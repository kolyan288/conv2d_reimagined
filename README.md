# conv2d_reimagined
Effective Deep Learning Models - Conv2d Reimagined: img2col, GEMM, Sparsity &amp; Quantization

## Обзор проекта
Этот проект посвящён исследованию и оптимизации реализации свёрточных слоёв в нейронных сетях. Основной фокус - преобразование операции свёртки в эффективное матричное умножение с последующим применением современных методов оптимизации.

## Ключевые цели:
* Исследование классической реализации nn.Conv2d в PyTorch

* Реализация преобразования свёртки в форму img2col для сведения к матричному умножению (GEMM)

* Применение методов оптимизации: спарсификация весов и квантование

* Экспериментальная оценка производительности и использования памяти

## Структура проекта
```txt
text
├── experiments/
│   ├── common/
│   │   ├── conv2d_img2col.py          # Базовая реализация Img2Col + GEMM
│   │   └── replace_conv_resnet.py     # Утилиты замены свёрточных слоёв
│   ├── conv2d_img2col_QAT.py          # Квантованный Img2Col (QAT)
│   └── [скоро появится] pruning/      # Эксперименты со спарсификацией
├── src/
│   ├── core/
│   │   └── latency.py                 # Утилиты измерения производительности
│   ├── models/
│   │   └── dummy.py                   # Тестовые модели для экспериментов
│   └── [скоро появится] quantization/ # Расширенные методы квантования
├── benchmarks/                         # Скрипты для запуска бенчмарков
├── results/                           # Результаты экспериментов
└── README.md
```
## Реализованные компоненты
### 1. Img2Col + GEMM преобразование
Файл: experiments/common/conv2d_img2col.py

Классы: Img2ColConvFunction, Conv2dImg2Col

Особенности: Автоматическое дифференцирование, поддержка различных параметров свёртки

### 2. Квантование (QAT - Quantization Aware Training)
Файл: experiments/conv2d_img2col_QAT.py

Поддержка: FP16 → INT8 квантование весов и активаций

Интеграция: Совместимость с PyTorch FX Graph Mode Quantization

### 3. Спарсификация

coming soon...

### 4. Измерение производительности
Файл: src/core/latency.py

Метрики: Время выполнения, использование GPU памяти

Платформы: CPU и GPU измерения с синхронизацией

### 5. Тестовая модель

coming soon...

## Быстрый старт
### Установка зависимостей
```bash
pip install torch torchvision numpy ...
```
### Базовое использование
```python
...
```
### Запуск экспериментов
```python
from src.core.latency import latency_gpu
from src.models.dummy import DummyModel
...
```
### Экспериментальная часть
План экспериментов:

Сравнение производительности: Forward/backward время для разных размеров ядер (3×3, 5×5, 7×7)

Анализ памяти: Использование GPU памяти для различных batch sizes

Оптимизации: Оценка эффекта от спарсификации и квантования

Интеграция: Тестирование в реальной модели

Прунинг 

Методы: Structured pruning

Квантование

Методы: 

### Воспроизведение результатов

coming soon...

## Технические детали
### Требования к оборудованию
* GPU с поддержкой CUDA (рекомендуется)
* PyTorch 1.9+
* Python 3.7+
...


Расширение поддерживаемых архитектур моделей

Улучшение системы измерений

Лицензия
[Указать лицензию проекта]
