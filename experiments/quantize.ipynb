{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import Sequential\n",
    "from common.conv2d_img2col import Conv2dImg2Col\n",
    "from torch.profiler import profile, record_function, ProfilerActivity, schedule\n",
    "import copy\n",
    "import os\n",
    "os.chdir('/workspaces/conv2d_reimagined')\n",
    "\n",
    "from experiments.conv2d_img2col_QAT import setup_qat_for_model, create_dummy_dataloader\n",
    "from torch.quantization.quantize_fx import convert_fx\n",
    "from torch.ao.quantization import get_default_qat_qconfig_mapping\n",
    "from src.models.dummy import DummyModel\n",
    "from src.core.latency import latency_cpu, latency_gpu, latency_cpu_profiler, latency_gpu_event\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dummy_model_input(batch=4, replace_conv2d=False):\n",
    "    model = DummyModel(replaced_conv=replace_conv2d)\n",
    "    input = torch.randn(batch, 3, 64, 64, requires_grad=False)\n",
    "\n",
    "    model.to(device)\n",
    "    input = input.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    lcpu = latency_cpu(model, input, warmup_n=10, benchmark_n=30)\n",
    "    lgpu = latency_gpu(model, input, warmup_n=10, benchmark_n=30)\n",
    "    lcpu_p1 = latency_cpu_profiler(model, input, warmup_n=10, benchmark_n=30)\n",
    "    print(lcpu_p1.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=5))\n",
    "    return model, input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    Conv2dImg2Col(3, 16, kernel_size=3, stride=1, padding=1, bias=True)\n",
    ")\n",
    "\n",
    "input = torch.randn(2, 3, 320, 320, requires_grad=True)\n",
    "\n",
    "model.to(device)\n",
    "input = input.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start GPU benchmark with input shape: torch.Size([2, 3, 320, 320]) cuda:0\n",
      "0.774ms +- 0.791ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4345855712890625, 0.053540829569101334)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latency_gpu(model, input, warmup_n=10, benchmark_n=10)\n",
    "latency_gpu_event(model, input, warmup=10, repeat=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start CPU benchmark with input shape: torch.Size([2, 3, 320, 320]) cpu\n",
      "13.079ms +- 2.903ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13.078514499648008, 2.9029896674571525)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latency_cpu(model, input, warmup_n=10, benchmark_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                                              Input Shapes  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------  \n",
      "          model_inference         7.20%       2.826ms       100.00%      39.260ms      39.260ms           0 b     -35.97 Mb             1                                                        []  \n",
      "         aten::contiguous         0.02%       9.000us        64.74%      25.416ms      25.416ms      21.09 Mb           0 b             1                              [[2, 320, 320, 3, 3, 3], []]  \n",
      "              aten::clone         0.16%      61.000us        64.71%      25.407ms      25.407ms      21.09 Mb           0 b             1                              [[2, 320, 320, 3, 3, 3], []]  \n",
      "              aten::copy_        64.43%      25.294ms        64.43%      25.294ms      25.294ms           0 b           0 b             1      [[2, 320, 320, 3, 3, 3], [2, 320, 320, 3, 3, 3], []]  \n",
      "                aten::pad         0.07%      27.000us        12.51%       4.912ms       4.912ms       2.37 Mb           0 b             1                            [[2, 3, 320, 320], [], [], []]  \n",
      "    aten::constant_pad_nd         0.93%     366.000us        12.44%       4.885ms       4.885ms       2.37 Mb           0 b             1                                [[2, 3, 320, 320], [], []]  \n",
      "                 aten::mm        12.36%       4.853ms        12.36%       4.854ms       4.854ms      12.50 Mb      12.50 Mb             1                                  [[204800, 27], [27, 16]]  \n",
      "              aten::fill_         9.82%       3.855ms         9.82%       3.855ms       3.855ms           0 b           0 b             1                                    [[2, 3, 322, 322], []]  \n",
      "               aten::add_         2.24%     881.000us         2.24%     881.000us     881.000us           0 b           0 b             1                               [[204800, 16], [1, 16], []]  \n",
      "              aten::copy_         0.86%     337.000us         0.86%     337.000us     337.000us           0 b           0 b             1                  [[2, 3, 320, 320], [2, 3, 320, 320], []]  \n",
      "              aten::empty         0.67%     264.000us         0.67%     264.000us     132.000us      23.47 Mb      23.47 Mb             2                                  [[], [], [], [], [], []]  \n",
      "             aten::narrow         0.09%      36.000us         0.20%      77.000us      77.000us           0 b           0 b             1                            [[2, 3, 322, 322], [], [], []]  \n",
      "            aten::permute         0.05%      21.000us         0.18%      72.000us      72.000us           0 b           0 b             1                              [[2, 3, 320, 320, 3, 3], []]  \n",
      "          aten::unsqueeze         0.16%      64.000us         0.18%      70.000us      70.000us           0 b           0 b             1                                                [[16], []]  \n",
      "         aten::empty_like         0.03%      13.000us         0.13%      52.000us      52.000us      21.09 Mb           0 b             1              [[2, 320, 320, 3, 3, 3], [], [], [], [], []]  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------  \n",
      "Self CPU time total: 39.260ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-10-24 15:07:02 9096:9096 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:07:03 9096:9096 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:07:03 9096:9096 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "input = input.to(\"cpu\")\n",
    "print(input.device)\n",
    "prof = latency_cpu_profiler(model, input, warmup_n=10, benchmark_n=30)\n",
    "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start CPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cpu\n",
      "53.059ms +- 3.258ms\n",
      "Start GPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cuda:0\n",
      "1.209ms +- 0.379ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/profiler/profiler.py:510: UserWarning: use_cuda is deprecated, use activities argument instead\n",
      "  warn(\"use_cuda is deprecated, use activities argument instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "        aten::mkldnn_convolution        67.71%      37.652ms        68.03%      37.830ms      12.610ms      28.00 Mb           0 b             3  \n",
      "                 aten::clamp_min        14.94%       8.309ms        14.94%       8.309ms       2.770ms      28.00 Mb      28.00 Mb             3  \n",
      "                 model_inference         5.02%       2.789ms       100.00%      55.607ms      55.607ms           0 b     -84.00 Mb             1  \n",
      "         aten::native_batch_norm         3.91%       2.172ms         4.00%       2.227ms     742.333us      28.00 Mb        -640 b             3  \n",
      "       aten::adaptive_avg_pool2d         2.38%       1.322ms         3.99%       2.217ms       2.217ms       4.00 Kb           0 b             1  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 55.607ms\n",
      "\n",
      "\n",
      "--- post training dynamic/weight_only quantization ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-10-24 15:46:58 22356:22356 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:46:58 22356:22356 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:46:58 22356:22356 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
      "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model, input = get_dummy_model_input(16)\n",
    "print(\"\\n--- post training dynamic/weight_only quantization ---\")\n",
    "\n",
    "# config = get_default_qat_qconfig_mapping(\"x86\") # qnnpack\n",
    "prepared_model = copy.deepcopy(model)\n",
    "prepared_model = setup_qat_for_model(prepared_model, input, config=None)\n",
    "prepared_model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start CPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cpu\n",
      "60.233ms +- 11.772ms\n",
      "\n",
      "--- after  convert_fx ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/profiler/profiler.py:510: UserWarning: use_cuda is deprecated, use activities argument instead\n",
      "  warn(\"use_cuda is deprecated, use activities argument instead\")\n",
      "STAGE:2025-10-24 15:47:03 22356:22356 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:47:03 22356:22356 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:47:03 22356:22356 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start CPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cpu\n",
      "18.577ms +- 2.144ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-10-24 15:47:04 22356:22356 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:47:04 22356:22356 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:47:04 22356:22356 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "lcpu = latency_cpu(prepared_model, input, warmup_n=10, benchmark_n=30)\n",
    "lcpu_p1 = latency_cpu_profiler(prepared_model, input, warmup_n=10, benchmark_n=30)\n",
    "\n",
    "prepared_model.cpu()\n",
    "save = convert_fx(prepared_model)\n",
    "print(\"\\n--- after  convert_fx ---\")\n",
    "lcpu = latency_cpu(save, input, warmup_n=10, benchmark_n=30)\n",
    "lcpu_p2 = latency_cpu_profiler(save, input, warmup_n=10, benchmark_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                                                                      Input Shapes  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                                        model_inference        18.10%      12.006ms       100.00%      66.340ms      66.340ms           0 b     -91.77 Mb             1                                                                                []  \n",
      "                                           aten::conv2d         0.02%      10.000us        39.77%      26.381ms      26.381ms      16.00 Mb           0 b             1                          [[16, 32, 64, 64], [64, 32, 3, 3], [64], [], [], [], []]  \n",
      "                                      aten::convolution         0.05%      32.000us        39.75%      26.371ms      26.371ms      16.00 Mb           0 b             1                  [[16, 32, 64, 64], [64, 32, 3, 3], [64], [], [], [], [], [], []]  \n",
      "                                     aten::_convolution         0.03%      20.000us        39.70%      26.339ms      26.339ms      16.00 Mb           0 b             1  [[16, 32, 64, 64], [64, 32, 3, 3], [64], [], [], [], [], [], [], [], [], [], []]  \n",
      "                               aten::mkldnn_convolution        39.60%      26.270ms        39.67%      26.319ms      26.319ms      16.00 Mb           0 b             1                          [[16, 32, 64, 64], [64, 32, 3, 3], [64], [], [], [], []]  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "Self CPU time total: 66.340ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    lcpu_p1.key_averages(group_by_input_shape=True).table(\n",
    "        sort_by=\"cpu_time_total\", row_limit=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                          Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "                  model_inference         8.10%       1.749ms       100.00%      21.591ms      21.591ms           0 b      -7.19 Mb             1                                    []  \n",
      "           quantized::conv2d_relu        49.47%      10.681ms        49.59%      10.706ms      10.706ms       4.00 Mb     -16.00 Mb             1        [[16, 32, 64, 64], [], [], []]  \n",
      "           quantized::conv2d_relu        26.46%       5.712ms        26.52%       5.725ms       5.725ms       2.00 Mb      -8.00 Mb             1        [[16, 16, 64, 64], [], [], []]  \n",
      "           quantized::conv2d_relu        12.66%       2.733ms        13.23%       2.856ms       2.856ms       1.00 Mb      -4.19 Mb             1         [[16, 3, 64, 64], [], [], []]  \n",
      "        aten::adaptive_avg_pool2d         0.05%      11.000us         1.55%     335.000us     335.000us       1.00 Kb           0 b             1                [[16, 64, 64, 64], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "Self CPU time total: 21.591ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    lcpu_p2.key_averages(group_by_input_shape=True).table(\n",
    "        sort_by=\"cpu_time_total\", row_limit=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Model with replaced conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start CPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cpu\n",
      "101.033ms +- 16.026ms\n",
      "Start GPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cuda:0\n",
      "2.602ms +- 0.774ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/profiler/profiler.py:510: UserWarning: use_cuda is deprecated, use activities argument instead\n",
      "  warn(\"use_cuda is deprecated, use activities argument instead\")\n",
      "STAGE:2025-10-24 15:53:01 23946:23946 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:53:02 23946:23946 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:53:02 23946:23946 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     aten::copy_        47.92%      81.330ms        47.92%      81.330ms      10.166ms           0 b           0 b             8  \n",
      "                        aten::mm        21.05%      35.730ms        21.05%      35.730ms      11.910ms      28.00 Mb      28.00 Mb             3  \n",
      "                 model_inference        11.04%      18.738ms       100.00%     169.729ms     169.729ms           0 b    -212.31 Mb             1  \n",
      "         aten::native_batch_norm         8.19%      13.896ms         8.27%      14.037ms       4.679ms      28.00 Mb        -704 b             3  \n",
      "                 aten::clamp_min         7.34%      12.451ms         7.34%      12.451ms       4.150ms      28.00 Mb      28.00 Mb             3  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 169.729ms\n",
      "\n",
      "\n",
      "--- post training dynamic/weight_only quantization ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model, input = get_dummy_model_input(16, replace_conv2d=True)\n",
    "print(\"\\n--- post training dynamic/weight_only quantization ---\")\n",
    "\n",
    "# config = get_default_qat_qconfig_mapping(\"x86\") # qnnpack\n",
    "prepared_model = copy.deepcopy(model)\n",
    "prepared_model = setup_qat_for_model(prepared_model, input, config=None)\n",
    "prepared_model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start CPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cpu\n",
      "857.720ms +- 68.400ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/profiler/profiler.py:510: UserWarning: use_cuda is deprecated, use activities argument instead\n",
      "  warn(\"use_cuda is deprecated, use activities argument instead\")\n",
      "STAGE:2025-10-24 15:53:52 23946:23946 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:53:53 23946:23946 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:53:53 23946:23946 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- after  convert_fx ---\n",
      "Start CPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cpu\n",
      "103.598ms +- 12.411ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-10-24 15:53:58 23946:23946 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:53:58 23946:23946 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:53:58 23946:23946 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "lcpu = latency_cpu(prepared_model, input, warmup_n=10, benchmark_n=30)\n",
    "lcpu_p1 = latency_cpu_profiler(prepared_model, input, warmup_n=10, benchmark_n=30)\n",
    "\n",
    "prepared_model.cpu()\n",
    "save = convert_fx(prepared_model)\n",
    "print(\"\\n--- after  convert_fx ---\")\n",
    "lcpu = latency_cpu(save, input, warmup_n=10, benchmark_n=30)\n",
    "lcpu_p2 = latency_cpu_profiler(save, input, warmup_n=10, benchmark_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ----------------------------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                                                Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ----------------------------------------------------------  \n",
      "                  model_inference         1.83%       2.051ms       100.00%     111.845ms     111.845ms           0 b    -241.49 Mb             1                                                          []  \n",
      "        aten::quantize_per_tensor         3.43%       3.832ms        26.74%      29.904ms      29.904ms      18.00 Mb     -54.00 Mb             1                        [[16, 32, 64, 64, 3, 3], [], [], []]  \n",
      "                         aten::mm        25.37%      28.372ms        25.37%      28.373ms      28.373ms      16.00 Mb      16.00 Mb             1                                   [[65536, 288], [288, 64]]  \n",
      "                 aten::contiguous         0.00%       5.000us        23.31%      26.068ms      26.068ms      72.00 Mb           0 b             1                                [[16, 32, 64, 64, 3, 3], []]  \n",
      "                      aten::clone         0.02%      20.000us        23.30%      26.063ms      26.063ms      72.00 Mb           0 b             1                                [[16, 32, 64, 64, 3, 3], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ----------------------------------------------------------  \n",
      "Self CPU time total: 111.845ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    lcpu_p2.key_averages(group_by_input_shape=True).table(\n",
    "        sort_by=\"cpu_time_total\", row_limit=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calibrated for static quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start CPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cpu\n",
      "43.836ms +- 8.974ms\n",
      "Start GPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cuda:0\n",
      "1.574ms +- 0.554ms\n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "        aten::mkldnn_convolution        68.80%      26.064ms        69.09%      26.174ms       8.725ms      28.00 Mb           0 b             3  \n",
      "                 aten::clamp_min        13.59%       5.148ms        13.59%       5.148ms       1.716ms      28.00 Mb      28.00 Mb             3  \n",
      "         aten::native_batch_norm         8.49%       3.215ms         8.58%       3.251ms       1.084ms      28.00 Mb        -704 b             3  \n",
      "                 model_inference         4.37%       1.655ms       100.00%      37.883ms      37.883ms           0 b     -84.00 Mb             1  \n",
      "                       aten::sum         2.42%     916.000us         2.43%     921.000us     921.000us           0 b           0 b             1  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 37.883ms\n",
      "\n",
      "\n",
      "--- post training dynamic/weight_only quantization ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-10-24 15:47:17 22356:22356 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:47:17 22356:22356 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:47:17 22356:22356 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
      "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/fake_quantize.py:343: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/ReduceAllOps.cpp:72.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- after  convert_fx ---\n",
      "Start CPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cpu\n",
      "18.673ms +- 2.247ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-10-24 15:47:18 22356:22356 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:47:18 22356:22356 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:47:18 22356:22356 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "model, input = get_dummy_model_input(16)\n",
    "print(\"\\n--- post training dynamic/weight_only quantization ---\")\n",
    "\n",
    "# config = get_default_qat_qconfig_mapping(\"x86\") # qnnpack\n",
    "prepared_model = copy.deepcopy(model)\n",
    "prepared_model = setup_qat_for_model(prepared_model, input, config=None)\n",
    "loader = create_dummy_dataloader()\n",
    "\n",
    "prepared_model = prepared_model.to(device)\n",
    "prepared_model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, target in loader: # dataloader transfer data to device\n",
    "        prepared_model(data)\n",
    "\n",
    "prepared_model.eval()\n",
    "prepared_model.cpu()\n",
    "save = convert_fx(prepared_model)\n",
    "print(\"\\n--- after  convert_fx ---\")\n",
    "lcpu = latency_cpu(save, input, warmup_n=10, benchmark_n=30)\n",
    "lcpu_p2 = latency_cpu_profiler(save, input, warmup_n=10, benchmark_n=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                          Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "                  model_inference         4.72%     867.000us       100.00%      18.361ms      18.361ms           0 b      -7.19 Mb             1                                    []  \n",
      "           quantized::conv2d_relu        54.57%      10.019ms        54.62%      10.029ms      10.029ms       4.00 Mb     -16.00 Mb             1        [[16, 32, 64, 64], [], [], []]  \n",
      "           quantized::conv2d_relu        22.96%       4.216ms        23.04%       4.230ms       4.230ms       2.00 Mb      -8.00 Mb             1        [[16, 16, 64, 64], [], [], []]  \n",
      "           quantized::conv2d_relu        13.50%       2.479ms        14.13%       2.595ms       2.595ms       1.00 Mb      -4.19 Mb             1         [[16, 3, 64, 64], [], [], []]  \n",
      "        aten::adaptive_avg_pool2d         0.04%       7.000us         1.72%     316.000us     316.000us       1.00 Kb           0 b             1                [[16, 64, 64, 64], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "Self CPU time total: 18.361ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lcpu_p2.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start export  via tracing with shape: input = torch.Size([16, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"start export  via tracing with shape: input =\", input.shape)\n",
    "save_traced = torch.jit.trace(save, input.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                          Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "                  model_inference        27.22%      21.425ms       100.00%      78.712ms      78.712ms           0 b        -640 b             1                                    []  \n",
      "                          forward         0.43%     335.000us        72.78%      57.287ms      57.287ms         640 b           0 b             1                 [[], [16, 3, 64, 64]]  \n",
      "           quantized::conv2d_relu        37.29%      29.352ms        37.36%      29.407ms      29.407ms       2.00 Mb     -18.00 Mb             1        [[16, 32, 64, 64], [], [], []]  \n",
      "           quantized::conv2d_relu        17.37%      13.672ms        17.44%      13.731ms      13.731ms       1.00 Mb      -9.00 Mb             1        [[16, 16, 64, 64], [], [], []]  \n",
      "           quantized::conv2d_relu        13.94%      10.974ms        15.63%      12.300ms      12.300ms     832.00 Kb      -4.38 Mb             1         [[16, 3, 64, 64], [], [], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "Self CPU time total: 78.712ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/profiler/profiler.py:510: UserWarning: use_cuda is deprecated, use activities argument instead\n",
      "  warn(\"use_cuda is deprecated, use activities argument instead\")\n",
      "STAGE:2025-10-24 15:19:22 12775:12775 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:19:22 12775:12775 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:19:22 12775:12775 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "lcpu_traced = latency_cpu_profiler(save_traced, input, warmup_n=10, benchmark_n=30)\n",
    "print(lcpu_traced.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                          Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "                  model_inference        16.26%       7.230ms       100.00%      44.458ms      44.458ms           0 b        -640 b             1                                    []  \n",
      "                          forward         1.99%     883.000us        83.74%      37.228ms      37.228ms         640 b        -160 b             1                 [[], [16, 3, 64, 64]]  \n",
      "           quantized::conv2d_relu        44.57%      19.814ms        44.64%      19.846ms      19.846ms       2.00 Mb     -18.00 Mb             1        [[16, 32, 64, 64], [], [], []]  \n",
      "           quantized::conv2d_relu        21.74%       9.664ms        21.82%       9.702ms       9.702ms       1.00 Mb      -9.00 Mb             1        [[16, 16, 64, 64], [], [], []]  \n",
      "           quantized::conv2d_relu        11.88%       5.282ms        12.78%       5.683ms       5.683ms     832.00 Kb      -4.38 Mb             1         [[16, 3, 64, 64], [], [], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "Self CPU time total: 44.458ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/profiler/profiler.py:510: UserWarning: use_cuda is deprecated, use activities argument instead\n",
      "  warn(\"use_cuda is deprecated, use activities argument instead\")\n",
      "STAGE:2025-10-24 15:21:00 12775:12775 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:21:00 12775:12775 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:21:00 12775:12775 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_scripted = torch.jit.script(save)\n",
    "lcpu_save_scripted = latency_cpu_profiler(save_scripted, input, warmup_n=10, benchmark_n=30)\n",
    "print(lcpu_save_scripted.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_onnx(save, example_input, \"traced\")\n",
    "torch.jit.save(save, \"model-jit-trace.pt\")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Model quantization complete!\")\n",
    "print(\"Models saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
