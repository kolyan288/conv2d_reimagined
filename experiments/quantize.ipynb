{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import Sequential\n",
    "from common.conv2d_img2col import Conv2dImg2Col\n",
    "from torch.profiler import profile, record_function, ProfilerActivity, schedule\n",
    "import copy\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import albumentations as A\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "\n",
    "os.chdir(\"/workspaces/conv2d_reimagined\")\n",
    "\n",
    "from experiments.conv2d_img2col_QAT import create_dummy_dataloader\n",
    "from torch.quantization.quantize_fx import convert_fx\n",
    "from torch.ao.quantization import get_default_qat_qconfig_mapping\n",
    "from src.models.dummy import DummyModel\n",
    "from src.core.latency import (\n",
    "    latency_cpu,\n",
    "    latency_gpu,\n",
    "    latency_cpu_profiler,\n",
    "    latency_gpu_event,\n",
    ")\n",
    "from src.core.quant import setup_qat_for_model\n",
    "from src.core.metric_writer import LatencyMetricsWriter\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy_model_input(batch=4, replace_conv2d=False):\n",
    "    model = DummyModel(replaced_conv=replace_conv2d)\n",
    "    input = torch.randn(batch, 3, 64, 64, requires_grad=False)\n",
    "\n",
    "    model.to(device)\n",
    "    input = input.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    lcpu = latency_cpu(model, input, warmup_n=10, benchmark_n=30)\n",
    "    lgpu = latency_gpu(model, input, warmup_n=10, benchmark_n=30)\n",
    "    lcpu_p1 = latency_cpu_profiler(model, input, warmup_n=10, benchmark_n=30)\n",
    "    print(lcpu_p1.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=5))\n",
    "    return model, input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    Conv2dImg2Col(3, 16, kernel_size=3, stride=1, padding=1, bias=True)\n",
    ")\n",
    "\n",
    "input = torch.randn(2, 3, 320, 320, requires_grad=True)\n",
    "\n",
    "model.to(device)\n",
    "input = input.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start GPU benchmark with input shape: torch.Size([2, 3, 320, 320]) cuda:0\n",
      "7.003ms +- 3.361ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12.413007736206055, 5.0858612060546875)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latency_gpu(model, input, warmup_n=10, benchmark_n=10)\n",
    "latency_gpu_event(model, input, warmup=10, repeat=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start CPU benchmark with input shape: torch.Size([2, 3, 320, 320]) cpu\n",
      "23.287ms +- 8.167ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(23.287114333288628, 8.16671405879791)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latency_cpu(model, input, warmup_n=10, benchmark_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                                              Input Shapes  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------  \n",
      "          model_inference         1.60%     343.000us       100.00%      21.477ms      21.477ms           0 b     -12.50 Mb             1                                                        []  \n",
      "      Img2ColConvFunction         4.31%     926.000us        98.40%      21.134ms      21.134ms      12.50 Mb     -23.47 Mb             1                   [[2, 3, 320, 320], [16, 3, 3, 3], [16]]  \n",
      "         aten::contiguous         0.03%       6.000us        70.27%      15.091ms      15.091ms      21.09 Mb           0 b             1                              [[2, 320, 320, 3, 3, 3], []]  \n",
      "              aten::clone         0.14%      30.000us        70.24%      15.085ms      15.085ms      21.09 Mb           0 b             1                              [[2, 320, 320, 3, 3, 3], []]  \n",
      "              aten::copy_        70.00%      15.034ms        70.00%      15.034ms      15.034ms           0 b           0 b             1      [[2, 320, 320, 3, 3, 3], [2, 320, 320, 3, 3, 3], []]  \n",
      "                 aten::mm        14.59%       3.133ms        14.59%       3.134ms       3.134ms      12.50 Mb      12.50 Mb             1                                  [[204800, 27], [27, 16]]  \n",
      "                aten::pad         0.08%      17.000us         4.56%     980.000us     980.000us       2.37 Mb           0 b             1                            [[2, 3, 320, 320], [], [], []]  \n",
      "    aten::constant_pad_nd         0.29%      62.000us         4.48%     963.000us     963.000us       2.37 Mb           0 b             1                                [[2, 3, 320, 320], [], []]  \n",
      "               aten::add_         3.69%     792.000us         3.69%     792.000us     792.000us           0 b           0 b             1                               [[204800, 16], [1, 16], []]  \n",
      "              aten::fill_         2.03%     437.000us         2.03%     437.000us     437.000us           0 b           0 b             1                                    [[2, 3, 322, 322], []]  \n",
      "              aten::copy_         1.76%     378.000us         1.76%     378.000us     378.000us           0 b           0 b             1                  [[2, 3, 320, 320], [2, 3, 320, 320], []]  \n",
      "             aten::unfold         0.30%      64.000us         0.33%      71.000us      71.000us           0 b           0 b             1                            [[2, 3, 322, 322], [], [], []]  \n",
      "             aten::narrow         0.08%      17.000us         0.21%      45.000us      45.000us           0 b           0 b             1                            [[2, 3, 322, 322], [], [], []]  \n",
      "              aten::empty         0.20%      44.000us         0.20%      44.000us      22.000us      23.47 Mb      23.47 Mb             2                                  [[], [], [], [], [], []]  \n",
      "          aten::unsqueeze         0.15%      33.000us         0.19%      40.000us      40.000us           0 b           0 b             1                                                [[16], []]  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------  \n",
      "Self CPU time total: 21.477ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-10-31 12:37:26 18839:18839 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-31 12:37:26 18839:18839 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-31 12:37:26 18839:18839 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "input = input.to(\"cpu\")\n",
    "print(input.device)\n",
    "prof = latency_cpu_profiler(model, input, warmup_n=10, benchmark_n=30)\n",
    "print(\n",
    "    prof.key_averages(group_by_input_shape=True).table(\n",
    "        sort_by=\"cpu_time_total\", row_limit=15\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start CPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cpu\n",
      "53.059ms +- 3.258ms\n",
      "Start GPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cuda:0\n",
      "1.209ms +- 0.379ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/profiler/profiler.py:510: UserWarning: use_cuda is deprecated, use activities argument instead\n",
      "  warn(\"use_cuda is deprecated, use activities argument instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "        aten::mkldnn_convolution        67.71%      37.652ms        68.03%      37.830ms      12.610ms      28.00 Mb           0 b             3  \n",
      "                 aten::clamp_min        14.94%       8.309ms        14.94%       8.309ms       2.770ms      28.00 Mb      28.00 Mb             3  \n",
      "                 model_inference         5.02%       2.789ms       100.00%      55.607ms      55.607ms           0 b     -84.00 Mb             1  \n",
      "         aten::native_batch_norm         3.91%       2.172ms         4.00%       2.227ms     742.333us      28.00 Mb        -640 b             3  \n",
      "       aten::adaptive_avg_pool2d         2.38%       1.322ms         3.99%       2.217ms       2.217ms       4.00 Kb           0 b             1  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 55.607ms\n",
      "\n",
      "\n",
      "--- post training dynamic/weight_only quantization ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-10-24 15:46:58 22356:22356 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:46:58 22356:22356 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:46:58 22356:22356 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
      "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model, input = get_dummy_model_input(16)\n",
    "print(\"\\n--- post training dynamic/weight_only quantization ---\")\n",
    "\n",
    "# config = get_default_qat_qconfig_mapping(\"x86\") # qnnpack\n",
    "prepared_model = copy.deepcopy(model)\n",
    "prepared_model = setup_qat_for_model(prepared_model, input, config=None)\n",
    "prepared_model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start CPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cpu\n",
      "60.233ms +- 11.772ms\n",
      "\n",
      "--- after  convert_fx ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/profiler/profiler.py:510: UserWarning: use_cuda is deprecated, use activities argument instead\n",
      "  warn(\"use_cuda is deprecated, use activities argument instead\")\n",
      "STAGE:2025-10-24 15:47:03 22356:22356 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:47:03 22356:22356 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:47:03 22356:22356 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start CPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cpu\n",
      "18.577ms +- 2.144ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-10-24 15:47:04 22356:22356 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:47:04 22356:22356 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:47:04 22356:22356 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "lcpu = latency_cpu(prepared_model, input, warmup_n=10, benchmark_n=30)\n",
    "lcpu_p1 = latency_cpu_profiler(prepared_model, input, warmup_n=10, benchmark_n=30)\n",
    "\n",
    "prepared_model.cpu()\n",
    "save = convert_fx(prepared_model)\n",
    "print(\"\\n--- after  convert_fx ---\")\n",
    "lcpu = latency_cpu(save, input, warmup_n=10, benchmark_n=30)\n",
    "lcpu_p2 = latency_cpu_profiler(save, input, warmup_n=10, benchmark_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                                                                      Input Shapes  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                                        model_inference        18.10%      12.006ms       100.00%      66.340ms      66.340ms           0 b     -91.77 Mb             1                                                                                []  \n",
      "                                           aten::conv2d         0.02%      10.000us        39.77%      26.381ms      26.381ms      16.00 Mb           0 b             1                          [[16, 32, 64, 64], [64, 32, 3, 3], [64], [], [], [], []]  \n",
      "                                      aten::convolution         0.05%      32.000us        39.75%      26.371ms      26.371ms      16.00 Mb           0 b             1                  [[16, 32, 64, 64], [64, 32, 3, 3], [64], [], [], [], [], [], []]  \n",
      "                                     aten::_convolution         0.03%      20.000us        39.70%      26.339ms      26.339ms      16.00 Mb           0 b             1  [[16, 32, 64, 64], [64, 32, 3, 3], [64], [], [], [], [], [], [], [], [], [], []]  \n",
      "                               aten::mkldnn_convolution        39.60%      26.270ms        39.67%      26.319ms      26.319ms      16.00 Mb           0 b             1                          [[16, 32, 64, 64], [64, 32, 3, 3], [64], [], [], [], []]  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "Self CPU time total: 66.340ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    lcpu_p1.key_averages(group_by_input_shape=True).table(\n",
    "        sort_by=\"cpu_time_total\", row_limit=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                          Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "                  model_inference         8.10%       1.749ms       100.00%      21.591ms      21.591ms           0 b      -7.19 Mb             1                                    []  \n",
      "           quantized::conv2d_relu        49.47%      10.681ms        49.59%      10.706ms      10.706ms       4.00 Mb     -16.00 Mb             1        [[16, 32, 64, 64], [], [], []]  \n",
      "           quantized::conv2d_relu        26.46%       5.712ms        26.52%       5.725ms       5.725ms       2.00 Mb      -8.00 Mb             1        [[16, 16, 64, 64], [], [], []]  \n",
      "           quantized::conv2d_relu        12.66%       2.733ms        13.23%       2.856ms       2.856ms       1.00 Mb      -4.19 Mb             1         [[16, 3, 64, 64], [], [], []]  \n",
      "        aten::adaptive_avg_pool2d         0.05%      11.000us         1.55%     335.000us     335.000us       1.00 Kb           0 b             1                [[16, 64, 64, 64], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "Self CPU time total: 21.591ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    lcpu_p2.key_averages(group_by_input_shape=True).table(\n",
    "        sort_by=\"cpu_time_total\", row_limit=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Model with replaced conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start CPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cpu\n",
      "101.033ms +- 16.026ms\n",
      "Start GPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cuda:0\n",
      "2.602ms +- 0.774ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/profiler/profiler.py:510: UserWarning: use_cuda is deprecated, use activities argument instead\n",
      "  warn(\"use_cuda is deprecated, use activities argument instead\")\n",
      "STAGE:2025-10-24 15:53:01 23946:23946 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:53:02 23946:23946 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:53:02 23946:23946 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     aten::copy_        47.92%      81.330ms        47.92%      81.330ms      10.166ms           0 b           0 b             8  \n",
      "                        aten::mm        21.05%      35.730ms        21.05%      35.730ms      11.910ms      28.00 Mb      28.00 Mb             3  \n",
      "                 model_inference        11.04%      18.738ms       100.00%     169.729ms     169.729ms           0 b    -212.31 Mb             1  \n",
      "         aten::native_batch_norm         8.19%      13.896ms         8.27%      14.037ms       4.679ms      28.00 Mb        -704 b             3  \n",
      "                 aten::clamp_min         7.34%      12.451ms         7.34%      12.451ms       4.150ms      28.00 Mb      28.00 Mb             3  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 169.729ms\n",
      "\n",
      "\n",
      "--- post training dynamic/weight_only quantization ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model, input = get_dummy_model_input(16, replace_conv2d=True)\n",
    "print(\"\\n--- post training dynamic/weight_only quantization ---\")\n",
    "\n",
    "# config = get_default_qat_qconfig_mapping(\"x86\") # qnnpack\n",
    "prepared_model = copy.deepcopy(model)\n",
    "prepared_model = setup_qat_for_model(prepared_model, input, config=None)\n",
    "prepared_model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start CPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cpu\n",
      "857.720ms +- 68.400ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/profiler/profiler.py:510: UserWarning: use_cuda is deprecated, use activities argument instead\n",
      "  warn(\"use_cuda is deprecated, use activities argument instead\")\n",
      "STAGE:2025-10-24 15:53:52 23946:23946 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:53:53 23946:23946 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:53:53 23946:23946 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- after  convert_fx ---\n",
      "Start CPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cpu\n",
      "103.598ms +- 12.411ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-10-24 15:53:58 23946:23946 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:53:58 23946:23946 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:53:58 23946:23946 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "lcpu = latency_cpu(prepared_model, input, warmup_n=10, benchmark_n=30)\n",
    "lcpu_p1 = latency_cpu_profiler(prepared_model, input, warmup_n=10, benchmark_n=30)\n",
    "\n",
    "prepared_model.cpu()\n",
    "save = convert_fx(prepared_model)\n",
    "print(\"\\n--- after  convert_fx ---\")\n",
    "lcpu = latency_cpu(save, input, warmup_n=10, benchmark_n=30)\n",
    "lcpu_p2 = latency_cpu_profiler(save, input, warmup_n=10, benchmark_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ----------------------------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                                                Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ----------------------------------------------------------  \n",
      "                  model_inference         1.83%       2.051ms       100.00%     111.845ms     111.845ms           0 b    -241.49 Mb             1                                                          []  \n",
      "        aten::quantize_per_tensor         3.43%       3.832ms        26.74%      29.904ms      29.904ms      18.00 Mb     -54.00 Mb             1                        [[16, 32, 64, 64, 3, 3], [], [], []]  \n",
      "                         aten::mm        25.37%      28.372ms        25.37%      28.373ms      28.373ms      16.00 Mb      16.00 Mb             1                                   [[65536, 288], [288, 64]]  \n",
      "                 aten::contiguous         0.00%       5.000us        23.31%      26.068ms      26.068ms      72.00 Mb           0 b             1                                [[16, 32, 64, 64, 3, 3], []]  \n",
      "                      aten::clone         0.02%      20.000us        23.30%      26.063ms      26.063ms      72.00 Mb           0 b             1                                [[16, 32, 64, 64, 3, 3], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ----------------------------------------------------------  \n",
      "Self CPU time total: 111.845ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    lcpu_p2.key_averages(group_by_input_shape=True).table(\n",
    "        sort_by=\"cpu_time_total\", row_limit=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calibrated for static quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start CPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cpu\n",
      "43.836ms +- 8.974ms\n",
      "Start GPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cuda:0\n",
      "1.574ms +- 0.554ms\n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "        aten::mkldnn_convolution        68.80%      26.064ms        69.09%      26.174ms       8.725ms      28.00 Mb           0 b             3  \n",
      "                 aten::clamp_min        13.59%       5.148ms        13.59%       5.148ms       1.716ms      28.00 Mb      28.00 Mb             3  \n",
      "         aten::native_batch_norm         8.49%       3.215ms         8.58%       3.251ms       1.084ms      28.00 Mb        -704 b             3  \n",
      "                 model_inference         4.37%       1.655ms       100.00%      37.883ms      37.883ms           0 b     -84.00 Mb             1  \n",
      "                       aten::sum         2.42%     916.000us         2.43%     921.000us     921.000us           0 b           0 b             1  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 37.883ms\n",
      "\n",
      "\n",
      "--- post training dynamic/weight_only quantization ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-10-24 15:47:17 22356:22356 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:47:17 22356:22356 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:47:17 22356:22356 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
      "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/fake_quantize.py:343: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/ReduceAllOps.cpp:72.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- after  convert_fx ---\n",
      "Start CPU benchmark with input shape: torch.Size([16, 3, 64, 64]) cpu\n",
      "18.673ms +- 2.247ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-10-24 15:47:18 22356:22356 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:47:18 22356:22356 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:47:18 22356:22356 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "model, input = get_dummy_model_input(16)\n",
    "print(\"\\n--- post training dynamic/weight_only quantization ---\")\n",
    "\n",
    "# config = get_default_qat_qconfig_mapping(\"x86\") # qnnpack\n",
    "prepared_model = copy.deepcopy(model)\n",
    "prepared_model = setup_qat_for_model(prepared_model, input, config=None)\n",
    "loader = create_dummy_dataloader()\n",
    "\n",
    "prepared_model = prepared_model.to(device)\n",
    "prepared_model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, target in loader:  # dataloader transfer data to device\n",
    "        prepared_model(data)\n",
    "\n",
    "prepared_model.eval()\n",
    "prepared_model.cpu()\n",
    "save = convert_fx(prepared_model)\n",
    "print(\"\\n--- after  convert_fx ---\")\n",
    "lcpu = latency_cpu(save, input, warmup_n=10, benchmark_n=30)\n",
    "lcpu_p2 = latency_cpu_profiler(save, input, warmup_n=10, benchmark_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                          Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "                  model_inference         4.72%     867.000us       100.00%      18.361ms      18.361ms           0 b      -7.19 Mb             1                                    []  \n",
      "           quantized::conv2d_relu        54.57%      10.019ms        54.62%      10.029ms      10.029ms       4.00 Mb     -16.00 Mb             1        [[16, 32, 64, 64], [], [], []]  \n",
      "           quantized::conv2d_relu        22.96%       4.216ms        23.04%       4.230ms       4.230ms       2.00 Mb      -8.00 Mb             1        [[16, 16, 64, 64], [], [], []]  \n",
      "           quantized::conv2d_relu        13.50%       2.479ms        14.13%       2.595ms       2.595ms       1.00 Mb      -4.19 Mb             1         [[16, 3, 64, 64], [], [], []]  \n",
      "        aten::adaptive_avg_pool2d         0.04%       7.000us         1.72%     316.000us     316.000us       1.00 Kb           0 b             1                [[16, 64, 64, 64], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "Self CPU time total: 18.361ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    lcpu_p2.key_averages(group_by_input_shape=True).table(\n",
    "        sort_by=\"cpu_time_total\", row_limit=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start export  via tracing with shape: input = torch.Size([16, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"start export  via tracing with shape: input =\", input.shape)\n",
    "save_traced = torch.jit.trace(save, input.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                          Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "                  model_inference        27.22%      21.425ms       100.00%      78.712ms      78.712ms           0 b        -640 b             1                                    []  \n",
      "                          forward         0.43%     335.000us        72.78%      57.287ms      57.287ms         640 b           0 b             1                 [[], [16, 3, 64, 64]]  \n",
      "           quantized::conv2d_relu        37.29%      29.352ms        37.36%      29.407ms      29.407ms       2.00 Mb     -18.00 Mb             1        [[16, 32, 64, 64], [], [], []]  \n",
      "           quantized::conv2d_relu        17.37%      13.672ms        17.44%      13.731ms      13.731ms       1.00 Mb      -9.00 Mb             1        [[16, 16, 64, 64], [], [], []]  \n",
      "           quantized::conv2d_relu        13.94%      10.974ms        15.63%      12.300ms      12.300ms     832.00 Kb      -4.38 Mb             1         [[16, 3, 64, 64], [], [], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "Self CPU time total: 78.712ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/profiler/profiler.py:510: UserWarning: use_cuda is deprecated, use activities argument instead\n",
      "  warn(\"use_cuda is deprecated, use activities argument instead\")\n",
      "STAGE:2025-10-24 15:19:22 12775:12775 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:19:22 12775:12775 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:19:22 12775:12775 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "lcpu_traced = latency_cpu_profiler(save_traced, input, warmup_n=10, benchmark_n=30)\n",
    "print(\n",
    "    lcpu_traced.key_averages(group_by_input_shape=True).table(\n",
    "        sort_by=\"cpu_time_total\", row_limit=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                          Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "                  model_inference        16.26%       7.230ms       100.00%      44.458ms      44.458ms           0 b        -640 b             1                                    []  \n",
      "                          forward         1.99%     883.000us        83.74%      37.228ms      37.228ms         640 b        -160 b             1                 [[], [16, 3, 64, 64]]  \n",
      "           quantized::conv2d_relu        44.57%      19.814ms        44.64%      19.846ms      19.846ms       2.00 Mb     -18.00 Mb             1        [[16, 32, 64, 64], [], [], []]  \n",
      "           quantized::conv2d_relu        21.74%       9.664ms        21.82%       9.702ms       9.702ms       1.00 Mb      -9.00 Mb             1        [[16, 16, 64, 64], [], [], []]  \n",
      "           quantized::conv2d_relu        11.88%       5.282ms        12.78%       5.683ms       5.683ms     832.00 Kb      -4.38 Mb             1         [[16, 3, 64, 64], [], [], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------  \n",
      "Self CPU time total: 44.458ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/profiler/profiler.py:510: UserWarning: use_cuda is deprecated, use activities argument instead\n",
      "  warn(\"use_cuda is deprecated, use activities argument instead\")\n",
      "STAGE:2025-10-24 15:21:00 12775:12775 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 15:21:00 12775:12775 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-24 15:21:00 12775:12775 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "save_scripted = torch.jit.script(save)\n",
    "lcpu_save_scripted = latency_cpu_profiler(\n",
    "    save_scripted, input, warmup_n=10, benchmark_n=30\n",
    ")\n",
    "print(\n",
    "    lcpu_save_scripted.key_averages(group_by_input_shape=True).table(\n",
    "        sort_by=\"cpu_time_total\", row_limit=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_onnx(save, example_input, \"traced\")\n",
    "torch.jit.save(save, \"model-jit-trace.pt\")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Model quantization complete!\")\n",
    "print(\"Models saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CamVidModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.camvid_segmentation_multiclass import (\n",
    "    get_dataloaders,\n",
    "    CamVidModel,\n",
    "    visualize_sample,\n",
    "    visualize_data,\n",
    "    train_val,\n",
    "    save_load_torch_model,\n",
    ")\n",
    "\n",
    "DATA_DIR = \"/workspaces/conv2d_reimagined/data/CamVid\"\n",
    "\n",
    "x_train_dir = os.path.join(DATA_DIR, \"train\")\n",
    "y_train_dir = os.path.join(DATA_DIR, \"trainannot\")\n",
    "\n",
    "x_valid_dir = os.path.join(DATA_DIR, \"val\")\n",
    "y_valid_dir = os.path.join(DATA_DIR, \"valannot\")\n",
    "\n",
    "x_test_dir = os.path.join(DATA_DIR, \"test\")\n",
    "y_test_dir = os.path.join(DATA_DIR, \"testannot\")\n",
    "\n",
    "train_loader, valid_loader, test_loader = get_dataloaders(\n",
    "    x_train_dir, y_train_dir, x_valid_dir, y_valid_dir, x_test_dir, y_test_dir, bs=4\n",
    ")\n",
    "\n",
    "# Some training hyperparameters\n",
    "\n",
    "EPOCHS = 50\n",
    "T_MAX = EPOCHS * len(train_loader)\n",
    "# Always include the background as a class\n",
    "OUT_CLASSES = len(train_loader.dataset.CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_data(x_train_dir, y_train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train simply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type     | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model   | FPN      | 25.6 M | train\n",
      "1 | loss_fn | DiceLoss | 0      | train\n",
      "---------------------------------------------\n",
      "25.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "25.6 M    Total params\n",
      "102.357   Total estimated model params size (MB)\n",
      "210       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 92/92 [00:56<00:00,  1.63it/s, v_num=13, valid_per_image_iou=0.561, valid_dataset_iou=0.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 92/92 [01:05<00:00,  1.41it/s, v_num=13, valid_per_image_iou=0.561, valid_dataset_iou=0.560]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 26/26 [00:12<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'valid_per_image_iou': 0.5607734322547913, 'valid_dataset_iou': 0.5604749917984009}]\n",
      "Testing DataLoader 0: 100%|██████████| 59/59 [00:25<00:00,  2.31it/s]\n",
      "[{'test_per_image_iou': 0.5147053003311157, 'test_dataset_iou': 0.5126190781593323}]\n"
     ]
    }
   ],
   "source": [
    "model = CamVidModel(\"FPN\", \"resnext50_32x4d\", in_channels=3, out_classes=OUT_CLASSES)\n",
    "valid_metrics, test_metrics, trainer = train_val(\n",
    "    model, train_loader, valid_loader, test_loader, max_epochs=1\n",
    ")\n",
    "\n",
    "save_load_torch_model(model, path=\"camvid_model_fp32.pt\")\n",
    "# model_loaded = save_load_torch_model(model, save=False, path = 'camvid_model_fp32.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 26/26 [00:10<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'valid_per_image_iou': 0.5607734322547913, 'valid_dataset_iou': 0.5604749917984009}]\n",
      "Testing DataLoader 0: 100%|██████████| 59/59 [00:23<00:00,  2.47it/s]\n",
      "[{'test_per_image_iou': 0.5147053003311157, 'test_dataset_iou': 0.5126190781593323}]\n"
     ]
    }
   ],
   "source": [
    "model = CamVidModel(\"FPN\", \"resnext50_32x4d\", in_channels=3, out_classes=OUT_CLASSES)\n",
    "model.eval()\n",
    "model_ = save_load_torch_model(model, path=\"camvid_model_fp32.pt\", save=False)\n",
    "valid_metrics, test_metrics, trainer = train_val(\n",
    "    model_, train_loader, valid_loader, test_loader, max_epochs=1, train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type     | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model   | FPN      | 25.6 M | train\n",
      "1 | loss_fn | DiceLoss | 0      | train\n",
      "---------------------------------------------\n",
      "25.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "25.6 M    Total params\n",
      "102.357   Total estimated model params size (MB)\n",
      "210       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 92/92 [01:00<00:00,  1.51it/s, v_num=0, valid_per_image_iou=0.783, valid_dataset_iou=0.782, train_per_image_iou=0.741, train_dataset_iou=0.732]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 92/92 [01:05<00:00,  1.40it/s, v_num=0, valid_per_image_iou=0.783, valid_dataset_iou=0.782, train_per_image_iou=0.741, train_dataset_iou=0.732]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 26/26 [00:10<00:00,  2.43it/s]\n",
      "[{'valid_per_image_iou': 0.7829864621162415, 'valid_dataset_iou': 0.7824925184249878}]\n"
     ]
    }
   ],
   "source": [
    "model = CamVidModel(\"FPN\", \"resnext50_32x4d\", in_channels=3, out_classes=OUT_CLASSES)\n",
    "valid_metrics, test_metrics, trainer = train_val(\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    test_loader,\n",
    "    max_epochs=20,\n",
    "    fp16=True,\n",
    "    log_every_n_steps=5,\n",
    ")\n",
    "\n",
    "trainer.model.eval()\n",
    "with torch.no_grad():\n",
    "    save_load_torch_model(trainer.model, path=\"camvid_model_fp16.pt\")\n",
    "\n",
    "# model = save_load_torch_model(model, path = 'camvid_model_fp16.pt', save=False)\n",
    "# model.half()\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 26/26 [00:13<00:00,  1.87it/s]\n",
      "[{'valid_per_image_iou': 0.7829864621162415, 'valid_dataset_iou': 0.7824925184249878}]\n"
     ]
    }
   ],
   "source": [
    "model = CamVidModel(\"FPN\", \"resnext50_32x4d\", in_channels=3, out_classes=OUT_CLASSES)\n",
    "model.eval()\n",
    "model = save_load_torch_model(model, path=\"camvid_model_fp16.pt\", save=False)\n",
    "valid_metrics, test_metrics, trainer = train_val(\n",
    "    model, train_loader, valid_loader, test_loader, max_epochs=1, fp16=True, train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 26/26 [00:13<00:00,  1.89it/s]\n",
      "[{'valid_per_image_iou': 0.5321038961410522, 'valid_dataset_iou': 0.5299340486526489}]\n"
     ]
    }
   ],
   "source": [
    "model = CamVidModel(\"FPN\", \"resnext50_32x4d\", in_channels=3, out_classes=OUT_CLASSES)\n",
    "model.eval()\n",
    "model = save_load_torch_model(model, path=\"camvid_model_fp16.pt\", save=False)\n",
    "model.half()\n",
    "valid_metrics, test_metrics, trainer = train_val(\n",
    "    model, train_loader, valid_loader, test_loader, max_epochs=1, fp16=True, train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-10-31 16:30:22 19376:19376 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-10-31 16:30:25 19376:19376 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-31 16:30:25 19376:19376 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start GPU benchmark with input shape: torch.Size([4, 3, 384, 480]) cuda:0\n",
      "56.661ms +- 17.074ms\n",
      "Metrics recorded for CamVidModel (batch_size=4)\n"
     ]
    }
   ],
   "source": [
    "model = CamVidModel(\"FPN\", \"resnext50_32x4d\", in_channels=3, out_classes=OUT_CLASSES)\n",
    "writer = LatencyMetricsWriter(\"q_model_latency_metrics.csv\")\n",
    "\n",
    "input, gt = next(iter(valid_loader))\n",
    "model = save_load_torch_model(model, path=\"camvid_model_fp16.pt\", save=False)\n",
    "model.eval()\n",
    "\n",
    "lcpu_p1 = latency_cpu_profiler(model, input, warmup_n=2, benchmark_n=5)\n",
    "gpu_mean_p1, gpu_std_p1 = latency_gpu(model, input, warmup_n=2, benchmark_n=5)\n",
    "\n",
    "# Record metrics for both models\n",
    "metrics_p1 = writer.record_metrics(\n",
    "    model=model,\n",
    "    model_name=\"CamVidModel\",\n",
    "    batch_size=input.shape[0],\n",
    "    precision=\"fp32\",\n",
    "    input_shape=input.shape,\n",
    "    cpu_profiler=lcpu_p1,\n",
    "    gpu_latency=(gpu_mean_p1, gpu_std_p1),\n",
    "    notes=\"Original model before conversion\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CamVidModel(\n",
       "  (model): FPN(\n",
       "    (encoder): ResNetEncoder(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): FPNDecoder(\n",
       "      (p5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (p4): FPNBlock(\n",
       "        (skip_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (p3): FPNBlock(\n",
       "        (skip_conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (p2): FPNBlock(\n",
       "        (skip_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (seg_blocks): ModuleList(\n",
       "        (0): SegmentationBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv3x3GNReLU(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Conv3x3GNReLU(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv3x3GNReLU(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): SegmentationBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv3x3GNReLU(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Conv3x3GNReLU(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2-3): 2 x SegmentationBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv3x3GNReLU(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (merge): MergeBlock()\n",
       "      (dropout): Dropout2d(p=0.2, inplace=True)\n",
       "    )\n",
       "    (segmentation_head): SegmentationHead(\n",
       "      (0): Conv2d(128, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): UpsamplingBilinear2d(scale_factor=4.0, mode='bilinear')\n",
       "      (2): Activation(\n",
       "        (activation): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): DiceLoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CamVidModel(\"FPN\", \"resnext50_32x4d\", in_channels=3, out_classes=OUT_CLASSES)\n",
    "writer = LatencyMetricsWriter(\"q_model_latency_metrics.csv\")\n",
    "\n",
    "input, gt = next(iter(valid_loader))\n",
    "model = save_load_torch_model(model, path=\"camvid_model_fp16.pt\", save=False)\n",
    "model.half()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start GPU benchmark with input shape: torch.Size([4, 3, 384, 480]) cuda:0\n",
      "33.684ms +- 9.564ms\n",
      "Metrics recorded for CamVidModel (batch_size=4)\n"
     ]
    }
   ],
   "source": [
    "lcpu_p1 = None\n",
    "gpu_mean_p1, gpu_std_p1 = latency_gpu(model, input, warmup_n=2, benchmark_n=5)\n",
    "\n",
    "# Record metrics for both models\n",
    "metrics_p1 = writer.record_metrics(\n",
    "    model=model,\n",
    "    model_name=\"CamVidModel\",\n",
    "    batch_size=input.shape[0],\n",
    "    precision=\"fp16\",\n",
    "    input_shape=input.shape,\n",
    "    cpu_profiler=lcpu_p1,\n",
    "    gpu_latency=(gpu_mean_p1, gpu_std_p1),\n",
    "    notes=\"slow_conv2d_cpu not implemented for 'Half'\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CamVidModel(\"FPN\", \"resnext50_32x4d\", in_channels=3, out_classes=OUT_CLASSES)\n",
    "# model_loaded = torch.nn.Sequential(*(list(model_loaded.children())[:-1]))\n",
    "input, gt = next(iter(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start CPU benchmark with input shape: torch.Size([4, 3, 384, 480]) cpu\n",
      "2958.581ms +- 298.977ms\n",
      "Start GPU benchmark with input shape: torch.Size([4, 3, 384, 480]) cuda:0\n",
      "60.842ms +- 9.170ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-10-31 13:51:33 10649:10649 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-10-31 13:51:37 10649:10649 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-31 13:51:37 10649:10649 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "lcpu = latency_cpu(model, input, warmup_n=2, benchmark_n=5)\n",
    "lgpu = latency_gpu(model, input, warmup_n=2, benchmark_n=5)\n",
    "lcpu_p1 = latency_cpu_profiler(model, input, warmup_n=2, benchmark_n=5)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ebable TF32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start GPU benchmark with input shape: torch.Size([4, 3, 384, 480]) cuda:0\n",
      "58.306ms +- 6.759ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enable TF32 for matrix multiplications\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# Enable TF32 for cuDNN (convolution operations)\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "model = CamVidModel(\"FPN\", \"resnext50_32x4d\", in_channels=3, out_classes=OUT_CLASSES)\n",
    "# model_loaded = torch.nn.Sequential(*(list(model_loaded.children())[:-1]))\n",
    "input, gt = next(iter(valid_loader))\n",
    "lgpu = latency_gpu(model, input, warmup_n=2, benchmark_n=5)\n",
    "\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example how select layers that we do /dont quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# do not quantize first / last layers\n",
    "dont_quantize = [\n",
    "    \"model.model.encoder.conv1\",\n",
    "    \"model.model.encoder.bn1\",\n",
    "    \"model.model.encoder.relu\",\n",
    "    \"model.model.encoder.maxpool\",\n",
    "    \"model.model.segmentation_head\",\n",
    "]\n",
    "\n",
    "from torch.ao.quantization import get_default_qconfig, QConfigMapping\n",
    "\n",
    "my_qconfig = get_default_qconfig(\"x86\")\n",
    "qconfig_mapping = QConfigMapping()\n",
    "qconfig_mapping = qconfig_mapping.set_global(my_qconfig)\n",
    "\n",
    "# Specify which layers to SKIP quantization for by setting their qconfig to None\n",
    "# You can do this by module name or type:\n",
    "for layer_name in dont_quantize:\n",
    "    qconfig_mapping = qconfig_mapping.set_module_name(layer_name, None)\n",
    "config = qconfig_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- after  convert_fx ---\n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                                               Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------  \n",
      "                  model_inference         3.40%      29.317ms       100.00%     862.018ms     862.018ms           0 b    -691.00 Mb             1                                                         []  \n",
      "                quantized::conv2d        13.24%     114.128ms        13.25%     114.248ms      38.083ms      22.50 Mb     -90.00 Mb             3                            [[4, 256, 96, 120], [], [], []]  \n",
      "                        aten::cat         3.03%      26.156ms         8.41%      72.469ms      36.234ms     112.50 Mb     -90.00 Mb             2                                                   [[], []]  \n",
      "           quantized::conv2d_relu         7.90%      68.114ms         7.92%      68.237ms       9.748ms      10.55 Mb     -42.19 Mb             7                            [[4, 1024, 24, 30], [], [], []]  \n",
      "                quantized::conv2d         7.45%      64.233ms         7.47%      64.359ms      12.872ms      23.91 Mb     -95.62 Mb             5                             [[4, 256, 48, 60], [], [], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------  \n",
      "Self CPU time total: 862.018ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prepared_model = copy.deepcopy(model)\n",
    "prepared_model.eval()\n",
    "prepared_model = setup_qat_for_model(prepared_model, input, config=None)\n",
    "\n",
    "\n",
    "# when dont quantize some layers\n",
    "prepared_model.cpu()\n",
    "save = convert_fx(prepared_model)\n",
    "\n",
    "lcpu = latency_cpu(save, input, warmup_n=10, benchmark_n=30)\n",
    "lcpu_p2 = latency_cpu_profiler(save, input, warmup_n=10, benchmark_n=30)\n",
    "\n",
    "# print(\n",
    "#     lcpu_p1.key_averages(group_by_input_shape=True).table(\n",
    "#         sort_by=\"cpu_time_total\", row_limit=5\n",
    "#     )\n",
    "# )\n",
    "print(\"\\n--- after  convert_fx ---\")\n",
    "print(\n",
    "    lcpu_p2.key_averages(group_by_input_shape=True).table(\n",
    "        sort_by=\"cpu_time_total\", row_limit=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.uint8\n",
      "torch.float32\n",
      "GraphModule(\n",
      "  (model): Module(\n",
      "    (encoder): Module(\n",
      "      (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=1.0, zero_point=0, padding=(3, 3))\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Module(\n",
      "        (0): Module(\n",
      "          (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (conv2): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), groups=32)\n",
      "          (conv3): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (downsample): Module(\n",
      "            (0): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (conv1): QuantizedConvReLU2d(256, 128, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (conv2): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), groups=32)\n",
      "          (conv3): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "        )\n",
      "        (2): Module(\n",
      "          (conv1): QuantizedConvReLU2d(256, 128, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (conv2): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), groups=32)\n",
      "          (conv3): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Module(\n",
      "        (0): Module(\n",
      "          (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1), groups=32)\n",
      "          (conv3): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (downsample): Module(\n",
      "            (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=1.0, zero_point=0)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (conv1): QuantizedConvReLU2d(512, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), groups=32)\n",
      "          (conv3): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "        )\n",
      "        (2): Module(\n",
      "          (conv1): QuantizedConvReLU2d(512, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), groups=32)\n",
      "          (conv3): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "        )\n",
      "        (3): Module(\n",
      "          (conv1): QuantizedConvReLU2d(512, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), groups=32)\n",
      "          (conv3): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Module(\n",
      "        (0): Module(\n",
      "          (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (conv2): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1), groups=32)\n",
      "          (conv3): QuantizedConv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (downsample): Module(\n",
      "            (0): QuantizedConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), scale=1.0, zero_point=0)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (conv1): QuantizedConvReLU2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (conv2): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), groups=32)\n",
      "          (conv3): QuantizedConv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "        )\n",
      "        (2): Module(\n",
      "          (conv1): QuantizedConvReLU2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (conv2): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), groups=32)\n",
      "          (conv3): QuantizedConv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "        )\n",
      "        (3): Module(\n",
      "          (conv1): QuantizedConvReLU2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (conv2): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), groups=32)\n",
      "          (conv3): QuantizedConv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "        )\n",
      "        (4): Module(\n",
      "          (conv1): QuantizedConvReLU2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (conv2): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), groups=32)\n",
      "          (conv3): QuantizedConv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "        )\n",
      "        (5): Module(\n",
      "          (conv1): QuantizedConvReLU2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (conv2): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), groups=32)\n",
      "          (conv3): QuantizedConv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Module(\n",
      "        (0): Module(\n",
      "          (conv1): QuantizedConvReLU2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (conv2): QuantizedConvReLU2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1), groups=32)\n",
      "          (conv3): QuantizedConv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (downsample): Module(\n",
      "            (0): QuantizedConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), scale=1.0, zero_point=0)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (conv1): QuantizedConvReLU2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (conv2): QuantizedConvReLU2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), groups=32)\n",
      "          (conv3): QuantizedConv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "        )\n",
      "        (2): Module(\n",
      "          (conv1): QuantizedConvReLU2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "          (conv2): QuantizedConvReLU2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), groups=32)\n",
      "          (conv3): QuantizedConv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): Module(\n",
      "      (p5): QuantizedConv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "      (p4): Module(\n",
      "        (skip_conv): QuantizedConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "      )\n",
      "      (p3): Module(\n",
      "        (skip_conv): QuantizedConv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "      )\n",
      "      (p2): Module(\n",
      "        (skip_conv): QuantizedConv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "      )\n",
      "      (seg_blocks): Module(\n",
      "        (0): Module(\n",
      "          (block): Module(\n",
      "            (0): Module(\n",
      "              (block): Module(\n",
      "                (0): QuantizedConv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
      "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Module(\n",
      "              (block): Module(\n",
      "                (0): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
      "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Module(\n",
      "              (block): Module(\n",
      "                (0): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
      "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (block): Module(\n",
      "            (0): Module(\n",
      "              (block): Module(\n",
      "                (0): QuantizedConv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
      "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Module(\n",
      "              (block): Module(\n",
      "                (0): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
      "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): Module(\n",
      "          (block): Module(\n",
      "            (0): Module(\n",
      "              (block): Module(\n",
      "                (0): QuantizedConv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
      "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Module(\n",
      "          (block): Module(\n",
      "            (0): Module(\n",
      "              (block): Module(\n",
      "                (0): QuantizedConv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
      "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout2d(p=0.2, inplace=True)\n",
      "    )\n",
      "    (segmentation_head): Module(\n",
      "      (0): QuantizedConv2d(128, 12, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
      "      (1): UpsamplingBilinear2d(scale_factor=4.0, mode='bilinear')\n",
      "      (2): Module(\n",
      "        (activation): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, image):\n",
      "    mean = self.mean\n",
      "    sub = image - mean;  image = mean = None\n",
      "    std = self.std\n",
      "    truediv = sub / std;  sub = std = None\n",
      "    _scale_0 = self._scale_0\n",
      "    _zero_point_0 = self._zero_point_0\n",
      "    quantize_per_tensor = torch.quantize_per_tensor(truediv, _scale_0, _zero_point_0, torch.quint8);  truediv = _scale_0 = _zero_point_0 = None\n",
      "    model_encoder_conv1 = self.model.encoder.conv1(quantize_per_tensor);  quantize_per_tensor = None\n",
      "    model_encoder_maxpool = self.model.encoder.maxpool(model_encoder_conv1);  model_encoder_conv1 = None\n",
      "    model_encoder_layer1_0_conv1 = getattr(self.model.encoder.layer1, \"0\").conv1(model_encoder_maxpool)\n",
      "    model_encoder_layer1_0_conv2 = getattr(self.model.encoder.layer1, \"0\").conv2(model_encoder_layer1_0_conv1);  model_encoder_layer1_0_conv1 = None\n",
      "    model_encoder_layer1_0_conv3 = getattr(self.model.encoder.layer1, \"0\").conv3(model_encoder_layer1_0_conv2);  model_encoder_layer1_0_conv2 = None\n",
      "    model_encoder_layer1_0_downsample_0 = getattr(getattr(self.model.encoder.layer1, \"0\").downsample, \"0\")(model_encoder_maxpool);  model_encoder_maxpool = None\n",
      "    model_encoder_layer1_0_relu_scale_0 = self.model_encoder_layer1_0_relu_scale_0\n",
      "    model_encoder_layer1_0_relu_zero_point_0 = self.model_encoder_layer1_0_relu_zero_point_0\n",
      "    add_relu = torch.ops.quantized.add_relu(model_encoder_layer1_0_conv3, model_encoder_layer1_0_downsample_0, model_encoder_layer1_0_relu_scale_0, model_encoder_layer1_0_relu_zero_point_0);  model_encoder_layer1_0_conv3 = model_encoder_layer1_0_downsample_0 = model_encoder_layer1_0_relu_scale_0 = model_encoder_layer1_0_relu_zero_point_0 = None\n",
      "    model_encoder_layer1_1_conv1 = getattr(self.model.encoder.layer1, \"1\").conv1(add_relu)\n",
      "    model_encoder_layer1_1_conv2 = getattr(self.model.encoder.layer1, \"1\").conv2(model_encoder_layer1_1_conv1);  model_encoder_layer1_1_conv1 = None\n",
      "    model_encoder_layer1_1_conv3 = getattr(self.model.encoder.layer1, \"1\").conv3(model_encoder_layer1_1_conv2);  model_encoder_layer1_1_conv2 = None\n",
      "    model_encoder_layer1_1_relu_scale_0 = self.model_encoder_layer1_1_relu_scale_0\n",
      "    model_encoder_layer1_1_relu_zero_point_0 = self.model_encoder_layer1_1_relu_zero_point_0\n",
      "    add_relu_1 = torch.ops.quantized.add_relu(model_encoder_layer1_1_conv3, add_relu, model_encoder_layer1_1_relu_scale_0, model_encoder_layer1_1_relu_zero_point_0);  model_encoder_layer1_1_conv3 = add_relu = model_encoder_layer1_1_relu_scale_0 = model_encoder_layer1_1_relu_zero_point_0 = None\n",
      "    model_encoder_layer1_2_conv1 = getattr(self.model.encoder.layer1, \"2\").conv1(add_relu_1)\n",
      "    model_encoder_layer1_2_conv2 = getattr(self.model.encoder.layer1, \"2\").conv2(model_encoder_layer1_2_conv1);  model_encoder_layer1_2_conv1 = None\n",
      "    model_encoder_layer1_2_conv3 = getattr(self.model.encoder.layer1, \"2\").conv3(model_encoder_layer1_2_conv2);  model_encoder_layer1_2_conv2 = None\n",
      "    model_encoder_layer1_2_relu_scale_0 = self.model_encoder_layer1_2_relu_scale_0\n",
      "    model_encoder_layer1_2_relu_zero_point_0 = self.model_encoder_layer1_2_relu_zero_point_0\n",
      "    add_relu_2 = torch.ops.quantized.add_relu(model_encoder_layer1_2_conv3, add_relu_1, model_encoder_layer1_2_relu_scale_0, model_encoder_layer1_2_relu_zero_point_0);  model_encoder_layer1_2_conv3 = add_relu_1 = model_encoder_layer1_2_relu_scale_0 = model_encoder_layer1_2_relu_zero_point_0 = None\n",
      "    model_encoder_layer2_0_conv1 = getattr(self.model.encoder.layer2, \"0\").conv1(add_relu_2)\n",
      "    model_encoder_layer2_0_conv2 = getattr(self.model.encoder.layer2, \"0\").conv2(model_encoder_layer2_0_conv1);  model_encoder_layer2_0_conv1 = None\n",
      "    model_encoder_layer2_0_conv3 = getattr(self.model.encoder.layer2, \"0\").conv3(model_encoder_layer2_0_conv2);  model_encoder_layer2_0_conv2 = None\n",
      "    model_encoder_layer2_0_downsample_0 = getattr(getattr(self.model.encoder.layer2, \"0\").downsample, \"0\")(add_relu_2)\n",
      "    model_encoder_layer2_0_relu_scale_0 = self.model_encoder_layer2_0_relu_scale_0\n",
      "    model_encoder_layer2_0_relu_zero_point_0 = self.model_encoder_layer2_0_relu_zero_point_0\n",
      "    add_relu_3 = torch.ops.quantized.add_relu(model_encoder_layer2_0_conv3, model_encoder_layer2_0_downsample_0, model_encoder_layer2_0_relu_scale_0, model_encoder_layer2_0_relu_zero_point_0);  model_encoder_layer2_0_conv3 = model_encoder_layer2_0_downsample_0 = model_encoder_layer2_0_relu_scale_0 = model_encoder_layer2_0_relu_zero_point_0 = None\n",
      "    model_encoder_layer2_1_conv1 = getattr(self.model.encoder.layer2, \"1\").conv1(add_relu_3)\n",
      "    model_encoder_layer2_1_conv2 = getattr(self.model.encoder.layer2, \"1\").conv2(model_encoder_layer2_1_conv1);  model_encoder_layer2_1_conv1 = None\n",
      "    model_encoder_layer2_1_conv3 = getattr(self.model.encoder.layer2, \"1\").conv3(model_encoder_layer2_1_conv2);  model_encoder_layer2_1_conv2 = None\n",
      "    model_encoder_layer2_1_relu_scale_0 = self.model_encoder_layer2_1_relu_scale_0\n",
      "    model_encoder_layer2_1_relu_zero_point_0 = self.model_encoder_layer2_1_relu_zero_point_0\n",
      "    add_relu_4 = torch.ops.quantized.add_relu(model_encoder_layer2_1_conv3, add_relu_3, model_encoder_layer2_1_relu_scale_0, model_encoder_layer2_1_relu_zero_point_0);  model_encoder_layer2_1_conv3 = add_relu_3 = model_encoder_layer2_1_relu_scale_0 = model_encoder_layer2_1_relu_zero_point_0 = None\n",
      "    model_encoder_layer2_2_conv1 = getattr(self.model.encoder.layer2, \"2\").conv1(add_relu_4)\n",
      "    model_encoder_layer2_2_conv2 = getattr(self.model.encoder.layer2, \"2\").conv2(model_encoder_layer2_2_conv1);  model_encoder_layer2_2_conv1 = None\n",
      "    model_encoder_layer2_2_conv3 = getattr(self.model.encoder.layer2, \"2\").conv3(model_encoder_layer2_2_conv2);  model_encoder_layer2_2_conv2 = None\n",
      "    model_encoder_layer2_2_relu_scale_0 = self.model_encoder_layer2_2_relu_scale_0\n",
      "    model_encoder_layer2_2_relu_zero_point_0 = self.model_encoder_layer2_2_relu_zero_point_0\n",
      "    add_relu_5 = torch.ops.quantized.add_relu(model_encoder_layer2_2_conv3, add_relu_4, model_encoder_layer2_2_relu_scale_0, model_encoder_layer2_2_relu_zero_point_0);  model_encoder_layer2_2_conv3 = add_relu_4 = model_encoder_layer2_2_relu_scale_0 = model_encoder_layer2_2_relu_zero_point_0 = None\n",
      "    model_encoder_layer2_3_conv1 = getattr(self.model.encoder.layer2, \"3\").conv1(add_relu_5)\n",
      "    model_encoder_layer2_3_conv2 = getattr(self.model.encoder.layer2, \"3\").conv2(model_encoder_layer2_3_conv1);  model_encoder_layer2_3_conv1 = None\n",
      "    model_encoder_layer2_3_conv3 = getattr(self.model.encoder.layer2, \"3\").conv3(model_encoder_layer2_3_conv2);  model_encoder_layer2_3_conv2 = None\n",
      "    model_encoder_layer2_3_relu_scale_0 = self.model_encoder_layer2_3_relu_scale_0\n",
      "    model_encoder_layer2_3_relu_zero_point_0 = self.model_encoder_layer2_3_relu_zero_point_0\n",
      "    add_relu_6 = torch.ops.quantized.add_relu(model_encoder_layer2_3_conv3, add_relu_5, model_encoder_layer2_3_relu_scale_0, model_encoder_layer2_3_relu_zero_point_0);  model_encoder_layer2_3_conv3 = add_relu_5 = model_encoder_layer2_3_relu_scale_0 = model_encoder_layer2_3_relu_zero_point_0 = None\n",
      "    model_encoder_layer3_0_conv1 = getattr(self.model.encoder.layer3, \"0\").conv1(add_relu_6)\n",
      "    model_encoder_layer3_0_conv2 = getattr(self.model.encoder.layer3, \"0\").conv2(model_encoder_layer3_0_conv1);  model_encoder_layer3_0_conv1 = None\n",
      "    model_encoder_layer3_0_conv3 = getattr(self.model.encoder.layer3, \"0\").conv3(model_encoder_layer3_0_conv2);  model_encoder_layer3_0_conv2 = None\n",
      "    model_encoder_layer3_0_downsample_0 = getattr(getattr(self.model.encoder.layer3, \"0\").downsample, \"0\")(add_relu_6)\n",
      "    model_encoder_layer3_0_relu_scale_0 = self.model_encoder_layer3_0_relu_scale_0\n",
      "    model_encoder_layer3_0_relu_zero_point_0 = self.model_encoder_layer3_0_relu_zero_point_0\n",
      "    add_relu_7 = torch.ops.quantized.add_relu(model_encoder_layer3_0_conv3, model_encoder_layer3_0_downsample_0, model_encoder_layer3_0_relu_scale_0, model_encoder_layer3_0_relu_zero_point_0);  model_encoder_layer3_0_conv3 = model_encoder_layer3_0_downsample_0 = model_encoder_layer3_0_relu_scale_0 = model_encoder_layer3_0_relu_zero_point_0 = None\n",
      "    model_encoder_layer3_1_conv1 = getattr(self.model.encoder.layer3, \"1\").conv1(add_relu_7)\n",
      "    model_encoder_layer3_1_conv2 = getattr(self.model.encoder.layer3, \"1\").conv2(model_encoder_layer3_1_conv1);  model_encoder_layer3_1_conv1 = None\n",
      "    model_encoder_layer3_1_conv3 = getattr(self.model.encoder.layer3, \"1\").conv3(model_encoder_layer3_1_conv2);  model_encoder_layer3_1_conv2 = None\n",
      "    model_encoder_layer3_1_relu_scale_0 = self.model_encoder_layer3_1_relu_scale_0\n",
      "    model_encoder_layer3_1_relu_zero_point_0 = self.model_encoder_layer3_1_relu_zero_point_0\n",
      "    add_relu_8 = torch.ops.quantized.add_relu(model_encoder_layer3_1_conv3, add_relu_7, model_encoder_layer3_1_relu_scale_0, model_encoder_layer3_1_relu_zero_point_0);  model_encoder_layer3_1_conv3 = add_relu_7 = model_encoder_layer3_1_relu_scale_0 = model_encoder_layer3_1_relu_zero_point_0 = None\n",
      "    model_encoder_layer3_2_conv1 = getattr(self.model.encoder.layer3, \"2\").conv1(add_relu_8)\n",
      "    model_encoder_layer3_2_conv2 = getattr(self.model.encoder.layer3, \"2\").conv2(model_encoder_layer3_2_conv1);  model_encoder_layer3_2_conv1 = None\n",
      "    model_encoder_layer3_2_conv3 = getattr(self.model.encoder.layer3, \"2\").conv3(model_encoder_layer3_2_conv2);  model_encoder_layer3_2_conv2 = None\n",
      "    model_encoder_layer3_2_relu_scale_0 = self.model_encoder_layer3_2_relu_scale_0\n",
      "    model_encoder_layer3_2_relu_zero_point_0 = self.model_encoder_layer3_2_relu_zero_point_0\n",
      "    add_relu_9 = torch.ops.quantized.add_relu(model_encoder_layer3_2_conv3, add_relu_8, model_encoder_layer3_2_relu_scale_0, model_encoder_layer3_2_relu_zero_point_0);  model_encoder_layer3_2_conv3 = add_relu_8 = model_encoder_layer3_2_relu_scale_0 = model_encoder_layer3_2_relu_zero_point_0 = None\n",
      "    model_encoder_layer3_3_conv1 = getattr(self.model.encoder.layer3, \"3\").conv1(add_relu_9)\n",
      "    model_encoder_layer3_3_conv2 = getattr(self.model.encoder.layer3, \"3\").conv2(model_encoder_layer3_3_conv1);  model_encoder_layer3_3_conv1 = None\n",
      "    model_encoder_layer3_3_conv3 = getattr(self.model.encoder.layer3, \"3\").conv3(model_encoder_layer3_3_conv2);  model_encoder_layer3_3_conv2 = None\n",
      "    model_encoder_layer3_3_relu_scale_0 = self.model_encoder_layer3_3_relu_scale_0\n",
      "    model_encoder_layer3_3_relu_zero_point_0 = self.model_encoder_layer3_3_relu_zero_point_0\n",
      "    add_relu_10 = torch.ops.quantized.add_relu(model_encoder_layer3_3_conv3, add_relu_9, model_encoder_layer3_3_relu_scale_0, model_encoder_layer3_3_relu_zero_point_0);  model_encoder_layer3_3_conv3 = add_relu_9 = model_encoder_layer3_3_relu_scale_0 = model_encoder_layer3_3_relu_zero_point_0 = None\n",
      "    model_encoder_layer3_4_conv1 = getattr(self.model.encoder.layer3, \"4\").conv1(add_relu_10)\n",
      "    model_encoder_layer3_4_conv2 = getattr(self.model.encoder.layer3, \"4\").conv2(model_encoder_layer3_4_conv1);  model_encoder_layer3_4_conv1 = None\n",
      "    model_encoder_layer3_4_conv3 = getattr(self.model.encoder.layer3, \"4\").conv3(model_encoder_layer3_4_conv2);  model_encoder_layer3_4_conv2 = None\n",
      "    model_encoder_layer3_4_relu_scale_0 = self.model_encoder_layer3_4_relu_scale_0\n",
      "    model_encoder_layer3_4_relu_zero_point_0 = self.model_encoder_layer3_4_relu_zero_point_0\n",
      "    add_relu_11 = torch.ops.quantized.add_relu(model_encoder_layer3_4_conv3, add_relu_10, model_encoder_layer3_4_relu_scale_0, model_encoder_layer3_4_relu_zero_point_0);  model_encoder_layer3_4_conv3 = add_relu_10 = model_encoder_layer3_4_relu_scale_0 = model_encoder_layer3_4_relu_zero_point_0 = None\n",
      "    model_encoder_layer3_5_conv1 = getattr(self.model.encoder.layer3, \"5\").conv1(add_relu_11)\n",
      "    model_encoder_layer3_5_conv2 = getattr(self.model.encoder.layer3, \"5\").conv2(model_encoder_layer3_5_conv1);  model_encoder_layer3_5_conv1 = None\n",
      "    model_encoder_layer3_5_conv3 = getattr(self.model.encoder.layer3, \"5\").conv3(model_encoder_layer3_5_conv2);  model_encoder_layer3_5_conv2 = None\n",
      "    model_encoder_layer3_5_relu_scale_0 = self.model_encoder_layer3_5_relu_scale_0\n",
      "    model_encoder_layer3_5_relu_zero_point_0 = self.model_encoder_layer3_5_relu_zero_point_0\n",
      "    add_relu_12 = torch.ops.quantized.add_relu(model_encoder_layer3_5_conv3, add_relu_11, model_encoder_layer3_5_relu_scale_0, model_encoder_layer3_5_relu_zero_point_0);  model_encoder_layer3_5_conv3 = add_relu_11 = model_encoder_layer3_5_relu_scale_0 = model_encoder_layer3_5_relu_zero_point_0 = None\n",
      "    model_encoder_layer4_0_conv1 = getattr(self.model.encoder.layer4, \"0\").conv1(add_relu_12)\n",
      "    model_encoder_layer4_0_conv2 = getattr(self.model.encoder.layer4, \"0\").conv2(model_encoder_layer4_0_conv1);  model_encoder_layer4_0_conv1 = None\n",
      "    model_encoder_layer4_0_conv3 = getattr(self.model.encoder.layer4, \"0\").conv3(model_encoder_layer4_0_conv2);  model_encoder_layer4_0_conv2 = None\n",
      "    model_encoder_layer4_0_downsample_0 = getattr(getattr(self.model.encoder.layer4, \"0\").downsample, \"0\")(add_relu_12)\n",
      "    model_encoder_layer4_0_relu_scale_0 = self.model_encoder_layer4_0_relu_scale_0\n",
      "    model_encoder_layer4_0_relu_zero_point_0 = self.model_encoder_layer4_0_relu_zero_point_0\n",
      "    add_relu_13 = torch.ops.quantized.add_relu(model_encoder_layer4_0_conv3, model_encoder_layer4_0_downsample_0, model_encoder_layer4_0_relu_scale_0, model_encoder_layer4_0_relu_zero_point_0);  model_encoder_layer4_0_conv3 = model_encoder_layer4_0_downsample_0 = model_encoder_layer4_0_relu_scale_0 = model_encoder_layer4_0_relu_zero_point_0 = None\n",
      "    model_encoder_layer4_1_conv1 = getattr(self.model.encoder.layer4, \"1\").conv1(add_relu_13)\n",
      "    model_encoder_layer4_1_conv2 = getattr(self.model.encoder.layer4, \"1\").conv2(model_encoder_layer4_1_conv1);  model_encoder_layer4_1_conv1 = None\n",
      "    model_encoder_layer4_1_conv3 = getattr(self.model.encoder.layer4, \"1\").conv3(model_encoder_layer4_1_conv2);  model_encoder_layer4_1_conv2 = None\n",
      "    model_encoder_layer4_1_relu_scale_0 = self.model_encoder_layer4_1_relu_scale_0\n",
      "    model_encoder_layer4_1_relu_zero_point_0 = self.model_encoder_layer4_1_relu_zero_point_0\n",
      "    add_relu_14 = torch.ops.quantized.add_relu(model_encoder_layer4_1_conv3, add_relu_13, model_encoder_layer4_1_relu_scale_0, model_encoder_layer4_1_relu_zero_point_0);  model_encoder_layer4_1_conv3 = add_relu_13 = model_encoder_layer4_1_relu_scale_0 = model_encoder_layer4_1_relu_zero_point_0 = None\n",
      "    model_encoder_layer4_2_conv1 = getattr(self.model.encoder.layer4, \"2\").conv1(add_relu_14)\n",
      "    model_encoder_layer4_2_conv2 = getattr(self.model.encoder.layer4, \"2\").conv2(model_encoder_layer4_2_conv1);  model_encoder_layer4_2_conv1 = None\n",
      "    model_encoder_layer4_2_conv3 = getattr(self.model.encoder.layer4, \"2\").conv3(model_encoder_layer4_2_conv2);  model_encoder_layer4_2_conv2 = None\n",
      "    model_encoder_layer4_2_relu_scale_0 = self.model_encoder_layer4_2_relu_scale_0\n",
      "    model_encoder_layer4_2_relu_zero_point_0 = self.model_encoder_layer4_2_relu_zero_point_0\n",
      "    add_relu_15 = torch.ops.quantized.add_relu(model_encoder_layer4_2_conv3, add_relu_14, model_encoder_layer4_2_relu_scale_0, model_encoder_layer4_2_relu_zero_point_0);  model_encoder_layer4_2_conv3 = add_relu_14 = model_encoder_layer4_2_relu_scale_0 = model_encoder_layer4_2_relu_zero_point_0 = None\n",
      "    model_decoder_p5 = self.model.decoder.p5(add_relu_15);  add_relu_15 = None\n",
      "    interpolate = torch.nn.functional.interpolate(model_decoder_p5, size = None, scale_factor = 2.0, mode = 'nearest', align_corners = None, recompute_scale_factor = None, antialias = False)\n",
      "    model_decoder_p4_skip_conv = self.model.decoder.p4.skip_conv(add_relu_12);  add_relu_12 = None\n",
      "    model_decoder_p4_scale_1 = self.model_decoder_p4_scale_1\n",
      "    model_decoder_p4_zero_point_1 = self.model_decoder_p4_zero_point_1\n",
      "    add_19 = torch.ops.quantized.add(interpolate, model_decoder_p4_skip_conv, model_decoder_p4_scale_1, model_decoder_p4_zero_point_1);  interpolate = model_decoder_p4_skip_conv = model_decoder_p4_scale_1 = model_decoder_p4_zero_point_1 = None\n",
      "    interpolate_1 = torch.nn.functional.interpolate(add_19, size = None, scale_factor = 2.0, mode = 'nearest', align_corners = None, recompute_scale_factor = None, antialias = False)\n",
      "    model_decoder_p3_skip_conv = self.model.decoder.p3.skip_conv(add_relu_6);  add_relu_6 = None\n",
      "    model_decoder_p3_scale_1 = self.model_decoder_p3_scale_1\n",
      "    model_decoder_p3_zero_point_1 = self.model_decoder_p3_zero_point_1\n",
      "    add_20 = torch.ops.quantized.add(interpolate_1, model_decoder_p3_skip_conv, model_decoder_p3_scale_1, model_decoder_p3_zero_point_1);  interpolate_1 = model_decoder_p3_skip_conv = model_decoder_p3_scale_1 = model_decoder_p3_zero_point_1 = None\n",
      "    interpolate_2 = torch.nn.functional.interpolate(add_20, size = None, scale_factor = 2.0, mode = 'nearest', align_corners = None, recompute_scale_factor = None, antialias = False)\n",
      "    model_decoder_p2_skip_conv = self.model.decoder.p2.skip_conv(add_relu_2);  add_relu_2 = None\n",
      "    model_decoder_p2_scale_1 = self.model_decoder_p2_scale_1\n",
      "    model_decoder_p2_zero_point_1 = self.model_decoder_p2_zero_point_1\n",
      "    add_21 = torch.ops.quantized.add(interpolate_2, model_decoder_p2_skip_conv, model_decoder_p2_scale_1, model_decoder_p2_zero_point_1);  interpolate_2 = model_decoder_p2_skip_conv = model_decoder_p2_scale_1 = model_decoder_p2_zero_point_1 = None\n",
      "    model_decoder_seg_blocks_0_block_0_block_0 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"0\").block, \"0\").block, \"0\")(model_decoder_p5);  model_decoder_p5 = None\n",
      "    dequantize_81 = model_decoder_seg_blocks_0_block_0_block_0.dequantize();  model_decoder_seg_blocks_0_block_0_block_0 = None\n",
      "    model_decoder_seg_blocks_0_block_0_block_1 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"0\").block, \"0\").block, \"1\")(dequantize_81);  dequantize_81 = None\n",
      "    model_decoder_seg_blocks_0_block_0_block_1_scale_0 = self.model_decoder_seg_blocks_0_block_0_block_1_scale_0\n",
      "    model_decoder_seg_blocks_0_block_0_block_1_zero_point_0 = self.model_decoder_seg_blocks_0_block_0_block_1_zero_point_0\n",
      "    quantize_per_tensor_82 = torch.quantize_per_tensor(model_decoder_seg_blocks_0_block_0_block_1, model_decoder_seg_blocks_0_block_0_block_1_scale_0, model_decoder_seg_blocks_0_block_0_block_1_zero_point_0, torch.quint8);  model_decoder_seg_blocks_0_block_0_block_1 = model_decoder_seg_blocks_0_block_0_block_1_scale_0 = model_decoder_seg_blocks_0_block_0_block_1_zero_point_0 = None\n",
      "    model_decoder_seg_blocks_0_block_0_block_2 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"0\").block, \"0\").block, \"2\")(quantize_per_tensor_82);  quantize_per_tensor_82 = None\n",
      "    interpolate_3 = torch.nn.functional.interpolate(model_decoder_seg_blocks_0_block_0_block_2, size = None, scale_factor = 2.0, mode = 'bilinear', align_corners = True, recompute_scale_factor = None, antialias = False);  model_decoder_seg_blocks_0_block_0_block_2 = None\n",
      "    model_decoder_seg_blocks_0_block_1_block_0 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"0\").block, \"1\").block, \"0\")(interpolate_3);  interpolate_3 = None\n",
      "    dequantize_85 = model_decoder_seg_blocks_0_block_1_block_0.dequantize();  model_decoder_seg_blocks_0_block_1_block_0 = None\n",
      "    model_decoder_seg_blocks_0_block_1_block_1 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"0\").block, \"1\").block, \"1\")(dequantize_85);  dequantize_85 = None\n",
      "    model_decoder_seg_blocks_0_block_1_block_1_scale_0 = self.model_decoder_seg_blocks_0_block_1_block_1_scale_0\n",
      "    model_decoder_seg_blocks_0_block_1_block_1_zero_point_0 = self.model_decoder_seg_blocks_0_block_1_block_1_zero_point_0\n",
      "    quantize_per_tensor_86 = torch.quantize_per_tensor(model_decoder_seg_blocks_0_block_1_block_1, model_decoder_seg_blocks_0_block_1_block_1_scale_0, model_decoder_seg_blocks_0_block_1_block_1_zero_point_0, torch.quint8);  model_decoder_seg_blocks_0_block_1_block_1 = model_decoder_seg_blocks_0_block_1_block_1_scale_0 = model_decoder_seg_blocks_0_block_1_block_1_zero_point_0 = None\n",
      "    model_decoder_seg_blocks_0_block_1_block_2 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"0\").block, \"1\").block, \"2\")(quantize_per_tensor_86);  quantize_per_tensor_86 = None\n",
      "    interpolate_4 = torch.nn.functional.interpolate(model_decoder_seg_blocks_0_block_1_block_2, size = None, scale_factor = 2.0, mode = 'bilinear', align_corners = True, recompute_scale_factor = None, antialias = False);  model_decoder_seg_blocks_0_block_1_block_2 = None\n",
      "    model_decoder_seg_blocks_0_block_2_block_0 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"0\").block, \"2\").block, \"0\")(interpolate_4);  interpolate_4 = None\n",
      "    dequantize_89 = model_decoder_seg_blocks_0_block_2_block_0.dequantize();  model_decoder_seg_blocks_0_block_2_block_0 = None\n",
      "    model_decoder_seg_blocks_0_block_2_block_1 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"0\").block, \"2\").block, \"1\")(dequantize_89);  dequantize_89 = None\n",
      "    model_decoder_seg_blocks_0_block_2_block_1_scale_0 = self.model_decoder_seg_blocks_0_block_2_block_1_scale_0\n",
      "    model_decoder_seg_blocks_0_block_2_block_1_zero_point_0 = self.model_decoder_seg_blocks_0_block_2_block_1_zero_point_0\n",
      "    quantize_per_tensor_90 = torch.quantize_per_tensor(model_decoder_seg_blocks_0_block_2_block_1, model_decoder_seg_blocks_0_block_2_block_1_scale_0, model_decoder_seg_blocks_0_block_2_block_1_zero_point_0, torch.quint8);  model_decoder_seg_blocks_0_block_2_block_1 = model_decoder_seg_blocks_0_block_2_block_1_scale_0 = model_decoder_seg_blocks_0_block_2_block_1_zero_point_0 = None\n",
      "    model_decoder_seg_blocks_0_block_2_block_2 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"0\").block, \"2\").block, \"2\")(quantize_per_tensor_90);  quantize_per_tensor_90 = None\n",
      "    interpolate_5 = torch.nn.functional.interpolate(model_decoder_seg_blocks_0_block_2_block_2, size = None, scale_factor = 2.0, mode = 'bilinear', align_corners = True, recompute_scale_factor = None, antialias = False);  model_decoder_seg_blocks_0_block_2_block_2 = None\n",
      "    model_decoder_seg_blocks_1_block_0_block_0 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"1\").block, \"0\").block, \"0\")(add_19);  add_19 = None\n",
      "    dequantize_93 = model_decoder_seg_blocks_1_block_0_block_0.dequantize();  model_decoder_seg_blocks_1_block_0_block_0 = None\n",
      "    model_decoder_seg_blocks_1_block_0_block_1 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"1\").block, \"0\").block, \"1\")(dequantize_93);  dequantize_93 = None\n",
      "    model_decoder_seg_blocks_1_block_0_block_1_scale_0 = self.model_decoder_seg_blocks_1_block_0_block_1_scale_0\n",
      "    model_decoder_seg_blocks_1_block_0_block_1_zero_point_0 = self.model_decoder_seg_blocks_1_block_0_block_1_zero_point_0\n",
      "    quantize_per_tensor_94 = torch.quantize_per_tensor(model_decoder_seg_blocks_1_block_0_block_1, model_decoder_seg_blocks_1_block_0_block_1_scale_0, model_decoder_seg_blocks_1_block_0_block_1_zero_point_0, torch.quint8);  model_decoder_seg_blocks_1_block_0_block_1 = model_decoder_seg_blocks_1_block_0_block_1_scale_0 = model_decoder_seg_blocks_1_block_0_block_1_zero_point_0 = None\n",
      "    model_decoder_seg_blocks_1_block_0_block_2 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"1\").block, \"0\").block, \"2\")(quantize_per_tensor_94);  quantize_per_tensor_94 = None\n",
      "    interpolate_6 = torch.nn.functional.interpolate(model_decoder_seg_blocks_1_block_0_block_2, size = None, scale_factor = 2.0, mode = 'bilinear', align_corners = True, recompute_scale_factor = None, antialias = False);  model_decoder_seg_blocks_1_block_0_block_2 = None\n",
      "    model_decoder_seg_blocks_1_block_1_block_0 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"1\").block, \"1\").block, \"0\")(interpolate_6);  interpolate_6 = None\n",
      "    dequantize_97 = model_decoder_seg_blocks_1_block_1_block_0.dequantize();  model_decoder_seg_blocks_1_block_1_block_0 = None\n",
      "    model_decoder_seg_blocks_1_block_1_block_1 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"1\").block, \"1\").block, \"1\")(dequantize_97);  dequantize_97 = None\n",
      "    model_decoder_seg_blocks_1_block_1_block_1_scale_0 = self.model_decoder_seg_blocks_1_block_1_block_1_scale_0\n",
      "    model_decoder_seg_blocks_1_block_1_block_1_zero_point_0 = self.model_decoder_seg_blocks_1_block_1_block_1_zero_point_0\n",
      "    quantize_per_tensor_98 = torch.quantize_per_tensor(model_decoder_seg_blocks_1_block_1_block_1, model_decoder_seg_blocks_1_block_1_block_1_scale_0, model_decoder_seg_blocks_1_block_1_block_1_zero_point_0, torch.quint8);  model_decoder_seg_blocks_1_block_1_block_1 = model_decoder_seg_blocks_1_block_1_block_1_scale_0 = model_decoder_seg_blocks_1_block_1_block_1_zero_point_0 = None\n",
      "    model_decoder_seg_blocks_1_block_1_block_2 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"1\").block, \"1\").block, \"2\")(quantize_per_tensor_98);  quantize_per_tensor_98 = None\n",
      "    interpolate_7 = torch.nn.functional.interpolate(model_decoder_seg_blocks_1_block_1_block_2, size = None, scale_factor = 2.0, mode = 'bilinear', align_corners = True, recompute_scale_factor = None, antialias = False);  model_decoder_seg_blocks_1_block_1_block_2 = None\n",
      "    model_decoder_seg_blocks_2_block_0_block_0 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"2\").block, \"0\").block, \"0\")(add_20);  add_20 = None\n",
      "    dequantize_101 = model_decoder_seg_blocks_2_block_0_block_0.dequantize();  model_decoder_seg_blocks_2_block_0_block_0 = None\n",
      "    model_decoder_seg_blocks_2_block_0_block_1 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"2\").block, \"0\").block, \"1\")(dequantize_101);  dequantize_101 = None\n",
      "    model_decoder_seg_blocks_2_block_0_block_1_scale_0 = self.model_decoder_seg_blocks_2_block_0_block_1_scale_0\n",
      "    model_decoder_seg_blocks_2_block_0_block_1_zero_point_0 = self.model_decoder_seg_blocks_2_block_0_block_1_zero_point_0\n",
      "    quantize_per_tensor_102 = torch.quantize_per_tensor(model_decoder_seg_blocks_2_block_0_block_1, model_decoder_seg_blocks_2_block_0_block_1_scale_0, model_decoder_seg_blocks_2_block_0_block_1_zero_point_0, torch.quint8);  model_decoder_seg_blocks_2_block_0_block_1 = model_decoder_seg_blocks_2_block_0_block_1_scale_0 = model_decoder_seg_blocks_2_block_0_block_1_zero_point_0 = None\n",
      "    model_decoder_seg_blocks_2_block_0_block_2 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"2\").block, \"0\").block, \"2\")(quantize_per_tensor_102);  quantize_per_tensor_102 = None\n",
      "    interpolate_8 = torch.nn.functional.interpolate(model_decoder_seg_blocks_2_block_0_block_2, size = None, scale_factor = 2.0, mode = 'bilinear', align_corners = True, recompute_scale_factor = None, antialias = False);  model_decoder_seg_blocks_2_block_0_block_2 = None\n",
      "    model_decoder_seg_blocks_3_block_0_block_0 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"3\").block, \"0\").block, \"0\")(add_21);  add_21 = None\n",
      "    dequantize_105 = model_decoder_seg_blocks_3_block_0_block_0.dequantize();  model_decoder_seg_blocks_3_block_0_block_0 = None\n",
      "    model_decoder_seg_blocks_3_block_0_block_1 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"3\").block, \"0\").block, \"1\")(dequantize_105);  dequantize_105 = None\n",
      "    model_decoder_seg_blocks_3_block_0_block_1_scale_0 = self.model_decoder_seg_blocks_3_block_0_block_1_scale_0\n",
      "    model_decoder_seg_blocks_3_block_0_block_1_zero_point_0 = self.model_decoder_seg_blocks_3_block_0_block_1_zero_point_0\n",
      "    quantize_per_tensor_106 = torch.quantize_per_tensor(model_decoder_seg_blocks_3_block_0_block_1, model_decoder_seg_blocks_3_block_0_block_1_scale_0, model_decoder_seg_blocks_3_block_0_block_1_zero_point_0, torch.quint8);  model_decoder_seg_blocks_3_block_0_block_1 = model_decoder_seg_blocks_3_block_0_block_1_scale_0 = model_decoder_seg_blocks_3_block_0_block_1_zero_point_0 = None\n",
      "    model_decoder_seg_blocks_3_block_0_block_2 = getattr(getattr(getattr(self.model.decoder.seg_blocks, \"3\").block, \"0\").block, \"2\")(quantize_per_tensor_106);  quantize_per_tensor_106 = None\n",
      "    stack = torch.stack([interpolate_5, interpolate_7, interpolate_8, model_decoder_seg_blocks_3_block_0_block_2]);  interpolate_5 = interpolate_7 = interpolate_8 = model_decoder_seg_blocks_3_block_0_block_2 = None\n",
      "    dequantize_108 = stack.dequantize();  stack = None\n",
      "    sum_1 = dequantize_108.sum(dim = 0);  dequantize_108 = None\n",
      "    model_decoder_dropout = self.model.decoder.dropout(sum_1);  sum_1 = None\n",
      "    model_decoder_dropout_scale_0 = self.model_decoder_dropout_scale_0\n",
      "    model_decoder_dropout_zero_point_0 = self.model_decoder_dropout_zero_point_0\n",
      "    quantize_per_tensor_109 = torch.quantize_per_tensor(model_decoder_dropout, model_decoder_dropout_scale_0, model_decoder_dropout_zero_point_0, torch.quint8);  model_decoder_dropout = model_decoder_dropout_scale_0 = model_decoder_dropout_zero_point_0 = None\n",
      "    model_segmentation_head_0 = getattr(self.model.segmentation_head, \"0\")(quantize_per_tensor_109);  quantize_per_tensor_109 = None\n",
      "    dequantize_110 = model_segmentation_head_0.dequantize();  model_segmentation_head_0 = None\n",
      "    model_segmentation_head_1 = getattr(self.model.segmentation_head, \"1\")(dequantize_110);  dequantize_110 = None\n",
      "    model_segmentation_head_1_scale_0 = self.model_segmentation_head_1_scale_0\n",
      "    model_segmentation_head_1_zero_point_0 = self.model_segmentation_head_1_zero_point_0\n",
      "    quantize_per_tensor_111 = torch.quantize_per_tensor(model_segmentation_head_1, model_segmentation_head_1_scale_0, model_segmentation_head_1_zero_point_0, torch.quint8);  model_segmentation_head_1 = model_segmentation_head_1_scale_0 = model_segmentation_head_1_zero_point_0 = None\n",
      "    model_segmentation_head_2_activation = getattr(self.model.segmentation_head, \"2\").activation(quantize_per_tensor_111);  quantize_per_tensor_111 = None\n",
      "    dequantize_112 = model_segmentation_head_2_activation.dequantize();  model_segmentation_head_2_activation = None\n",
      "    return dequantize_112\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "print(input.dtype)\n",
    "print(save(input).dtype)\n",
    "print(save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CamVidModel(\n",
       "  (model): GraphModule(\n",
       "    (activation_post_process_0): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (encoder): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Module(\n",
       "        (0): Module(\n",
       "          (conv1): ConvReLU2d(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvReLU2d(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (downsample): Module(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Module(\n",
       "          (conv1): ConvReLU2d(\n",
       "            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvReLU2d(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Module(\n",
       "          (conv1): ConvReLU2d(\n",
       "            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvReLU2d(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Module(\n",
       "        (0): Module(\n",
       "          (conv1): ConvReLU2d(\n",
       "            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvReLU2d(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32)\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (downsample): Module(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "          )\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Module(\n",
       "          (conv1): ConvReLU2d(\n",
       "            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvReLU2d(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Module(\n",
       "          (conv1): ConvReLU2d(\n",
       "            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvReLU2d(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Module(\n",
       "          (conv1): ConvReLU2d(\n",
       "            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvReLU2d(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Module(\n",
       "        (0): Module(\n",
       "          (conv1): ConvReLU2d(\n",
       "            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvReLU2d(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32)\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (downsample): Module(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2))\n",
       "          )\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Module(\n",
       "          (conv1): ConvReLU2d(\n",
       "            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvReLU2d(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Module(\n",
       "          (conv1): ConvReLU2d(\n",
       "            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvReLU2d(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Module(\n",
       "          (conv1): ConvReLU2d(\n",
       "            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvReLU2d(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Module(\n",
       "          (conv1): ConvReLU2d(\n",
       "            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvReLU2d(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Module(\n",
       "          (conv1): ConvReLU2d(\n",
       "            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvReLU2d(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Module(\n",
       "        (0): Module(\n",
       "          (conv1): ConvReLU2d(\n",
       "            (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvReLU2d(\n",
       "            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32)\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (downsample): Module(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2))\n",
       "          )\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Module(\n",
       "          (conv1): ConvReLU2d(\n",
       "            (0): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvReLU2d(\n",
       "            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Module(\n",
       "          (conv1): ConvReLU2d(\n",
       "            (0): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvReLU2d(\n",
       "            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (activation_post_process_1): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_2): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_3): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_4): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_5): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_6): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_7): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_8): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_9): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_10): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_11): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_12): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_13): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_14): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_15): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_16): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_17): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_18): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_19): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_20): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_21): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_22): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_23): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_24): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_25): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_26): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_27): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_28): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_29): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_30): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_31): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_32): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_33): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_34): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_35): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_36): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_37): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_38): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_39): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_40): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_41): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_42): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_43): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_44): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_45): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_46): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_47): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_48): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_49): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_50): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_51): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_52): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_53): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_54): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_55): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_56): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_57): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_58): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_59): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_60): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_61): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_62): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_63): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_64): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_65): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_66): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_67): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_68): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_69): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_70): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (decoder): Module(\n",
       "      (p5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (p4): Module(\n",
       "        (skip_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (p3): Module(\n",
       "        (skip_conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (p2): Module(\n",
       "        (skip_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (seg_blocks): Module(\n",
       "        (0): Module(\n",
       "          (block): Module(\n",
       "            (0): Module(\n",
       "              (block): Module(\n",
       "                (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Module(\n",
       "              (block): Module(\n",
       "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Module(\n",
       "              (block): Module(\n",
       "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (block): Module(\n",
       "            (0): Module(\n",
       "              (block): Module(\n",
       "                (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Module(\n",
       "              (block): Module(\n",
       "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Module(\n",
       "          (block): Module(\n",
       "            (0): Module(\n",
       "              (block): Module(\n",
       "                (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Module(\n",
       "          (block): Module(\n",
       "            (0): Module(\n",
       "              (block): Module(\n",
       "                (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout2d(p=0.2, inplace=True)\n",
       "    )\n",
       "    (activation_post_process_71): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_72): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_73): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_74): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_75): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_76): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_77): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_78): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_79): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_80): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_81): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_82): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_83): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_84): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_85): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_86): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_87): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_88): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_89): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_90): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_91): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_92): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_93): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_94): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_95): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_96): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_97): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_98): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_99): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_100): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_101): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_102): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_103): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_104): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_105): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_106): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_107): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_108): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_109): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (segmentation_head): Module(\n",
       "      (0): Conv2d(128, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): UpsamplingBilinear2d(scale_factor=4.0, mode='bilinear')\n",
       "      (2): Module(\n",
       "        (activation): Identity()\n",
       "      )\n",
       "    )\n",
       "    (activation_post_process_110): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_111): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process_112): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): DiceLoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.ao.quantization import get_default_qat_qconfig_mapping\n",
    "config = get_default_qat_qconfig_mapping(\"x86\")\n",
    "# fbgemm (onednn), qnnpack (xnnpack)\n",
    "\n",
    "prepared_model = copy.deepcopy(model)\n",
    "prepared_model.eval()\n",
    "prepared_model = prepared_model.setup_qat(input, config)\n",
    "prepared_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type        | Params | Mode\n",
      "-----------------------------------------------\n",
      "0 | model   | GraphModule | 25.6 M | eval\n",
      "1 | loss_fn | DiceLoss    | 0      | eval\n",
      "-----------------------------------------------\n",
      "25.5 M    Trainable params\n",
      "34.1 K    Non-trainable params\n",
      "25.6 M    Total params\n",
      "102.221   Total estimated model params size (MB)\n",
      "275       Modules in train mode\n",
      "102       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 92/92 [01:03<00:00,  1.46it/s, v_num=10, valid_per_image_iou=0.392, valid_dataset_iou=0.391]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 92/92 [01:14<00:00,  1.24it/s, v_num=10, valid_per_image_iou=0.392, valid_dataset_iou=0.391]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=1, log_every_n_steps=1, callbacks=[])\n",
    "trainer.fit(\n",
    "    prepared_model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=valid_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.model.eval()\n",
    "trainer.model.model.cpu()\n",
    "save = convert_fx(trainer.model.model.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 4.0902,  4.2680,  4.4459,  ...,  4.6237,  4.4459,  4.2680],\n",
       "          [ 4.0902,  4.2680,  4.6237,  ...,  4.8015,  4.6237,  4.4459],\n",
       "          [ 4.0902,  4.4459,  4.6237,  ...,  4.8015,  4.6237,  4.4459],\n",
       "          ...,\n",
       "          [ 3.9124,  4.0902,  4.2680,  ...,  4.2680,  4.2680,  4.0902],\n",
       "          [ 3.9124,  4.0902,  4.2680,  ...,  4.4459,  4.2680,  4.2680],\n",
       "          [ 4.0902,  4.0902,  4.2680,  ...,  4.4459,  4.4459,  4.2680]],\n",
       "\n",
       "         [[-1.6005, -1.6005, -1.6005,  ..., -1.9562, -2.1340, -2.3118],\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -2.1340, -2.1340, -2.3118],\n",
       "          [-1.6005, -1.7783, -1.7783,  ..., -2.1340, -2.3118, -2.3118],\n",
       "          ...,\n",
       "          [-2.8454, -2.8454, -2.8454,  ..., -2.4897, -2.4897, -2.3118],\n",
       "          [-2.6675, -2.6675, -2.6675,  ..., -2.4897, -2.3118, -2.3118],\n",
       "          [-2.6675, -2.6675, -2.6675,  ..., -2.4897, -2.3118, -2.3118]],\n",
       "\n",
       "         [[ 0.5335,  0.5335,  0.5335,  ..., -0.3557, -0.3557, -0.5335],\n",
       "          [ 0.5335,  0.5335,  0.5335,  ..., -0.1778, -0.3557, -0.3557],\n",
       "          [ 0.5335,  0.5335,  0.5335,  ..., -0.1778, -0.1778, -0.3557],\n",
       "          ...,\n",
       "          [-0.5335, -0.5335, -0.5335,  ..., -0.1778, -0.1778, -0.1778],\n",
       "          [-0.5335, -0.5335, -0.5335,  ..., -0.3557, -0.3557, -0.3557],\n",
       "          [-0.5335, -0.5335, -0.5335,  ..., -0.5335, -0.5335, -0.5335]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.1778, -0.3557, -0.3557,  ..., -0.1778,  0.0000,  0.1778],\n",
       "          [-0.3557, -0.3557, -0.3557,  ..., -0.1778,  0.0000,  0.1778],\n",
       "          [-0.3557, -0.3557, -0.5335,  ..., -0.1778, -0.1778,  0.0000],\n",
       "          ...,\n",
       "          [ 0.3557,  0.1778,  0.1778,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.3557,  0.3557,  0.1778,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.5335,  0.3557,  0.1778,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-1.6005, -1.6005, -1.4227,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-1.4227, -1.4227, -1.4227,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-1.4227, -1.2448, -1.2448,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          ...,\n",
       "          [-1.2448, -1.2448, -1.4227,  ..., -1.0670, -1.0670, -1.0670],\n",
       "          [-1.2448, -1.4227, -1.4227,  ..., -1.0670, -1.0670, -0.8892],\n",
       "          [-1.4227, -1.4227, -1.4227,  ..., -1.0670, -1.0670, -0.8892]],\n",
       "\n",
       "         [[-2.3118, -2.3118, -2.3118,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-2.3118, -2.3118, -2.3118,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-2.3118, -2.3118, -2.3118,  ..., -1.9562, -1.9562, -1.9562],\n",
       "          ...,\n",
       "          [-1.6005, -1.4227, -1.4227,  ..., -1.6005, -1.6005, -1.6005],\n",
       "          [-1.6005, -1.4227, -1.4227,  ..., -1.6005, -1.6005, -1.6005],\n",
       "          [-1.6005, -1.6005, -1.4227,  ..., -1.6005, -1.6005, -1.6005]]],\n",
       "\n",
       "\n",
       "        [[[ 4.4459,  4.6237,  4.8015,  ...,  4.6237,  4.4459,  4.2680],\n",
       "          [ 4.6237,  4.6237,  4.8015,  ...,  4.6237,  4.4459,  4.2680],\n",
       "          [ 4.6237,  4.8015,  4.9794,  ...,  4.6237,  4.4459,  4.2680],\n",
       "          ...,\n",
       "          [ 4.8015,  4.8015,  4.8015,  ...,  4.4459,  4.4459,  4.4459],\n",
       "          [ 4.8015,  4.8015,  4.8015,  ...,  4.4459,  4.4459,  4.4459],\n",
       "          [ 4.8015,  4.8015,  4.8015,  ...,  4.4459,  4.4459,  4.4459]],\n",
       "\n",
       "         [[-1.6005, -1.6005, -1.6005,  ..., -1.9562, -2.1340, -2.3118],\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -2.1340, -2.1340, -2.3118],\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -2.1340, -2.3118, -2.3118],\n",
       "          ...,\n",
       "          [-2.6675, -2.6675, -2.6675,  ..., -2.6675, -2.4897, -2.4897],\n",
       "          [-2.4897, -2.4897, -2.4897,  ..., -2.6675, -2.4897, -2.4897],\n",
       "          [-2.4897, -2.4897, -2.4897,  ..., -2.6675, -2.4897, -2.4897]],\n",
       "\n",
       "         [[ 0.7113,  0.5335,  0.5335,  ..., -0.1778, -0.1778, -0.1778],\n",
       "          [ 0.7113,  0.7113,  0.5335,  ...,  0.0000,  0.0000, -0.1778],\n",
       "          [ 0.7113,  0.7113,  0.7113,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [-0.3557, -0.3557, -0.3557,  ...,  0.0000,  0.0000,  0.1778],\n",
       "          [-0.3557, -0.3557, -0.3557,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.5335, -0.5335, -0.5335,  ..., -0.1778,  0.0000,  0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1778,  0.1778,  0.1778,  ..., -0.1778, -0.1778,  0.0000],\n",
       "          [ 0.1778,  0.0000,  0.0000,  ..., -0.3557, -0.1778, -0.1778],\n",
       "          [ 0.0000,  0.0000, -0.1778,  ..., -0.3557, -0.3557, -0.1778],\n",
       "          ...,\n",
       "          [ 0.1778,  0.1778,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.1778,  0.1778,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.1778,  0.1778,  0.1778,  ..., -0.1778,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-1.6005, -1.6005, -1.6005,  ..., -1.9562, -1.9562, -1.9562],\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -1.9562, -1.9562, -1.9562],\n",
       "          [-1.4227, -1.6005, -1.6005,  ..., -1.9562, -1.9562, -1.9562],\n",
       "          ...,\n",
       "          [-1.0670, -1.0670, -1.0670,  ..., -0.7113, -0.7113, -0.7113],\n",
       "          [-1.0670, -1.0670, -1.0670,  ..., -0.7113, -0.7113, -0.7113],\n",
       "          [-1.0670, -1.0670, -1.0670,  ..., -0.7113, -0.7113, -0.7113]],\n",
       "\n",
       "         [[-2.3118, -2.3118, -2.3118,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-2.3118, -2.3118, -2.1340,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-2.3118, -2.1340, -2.1340,  ..., -1.7783, -1.9562, -1.9562],\n",
       "          ...,\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -1.9562, -1.7783, -1.7783]]],\n",
       "\n",
       "\n",
       "        [[[ 4.2680,  4.4459,  4.4459,  ...,  4.8015,  4.6237,  4.4459],\n",
       "          [ 4.2680,  4.4459,  4.6237,  ...,  4.8015,  4.8015,  4.6237],\n",
       "          [ 4.0902,  4.4459,  4.6237,  ...,  4.9794,  4.8015,  4.6237],\n",
       "          ...,\n",
       "          [ 4.0902,  4.0902,  4.2680,  ...,  4.2680,  4.2680,  4.2680],\n",
       "          [ 4.0902,  4.0902,  4.2680,  ...,  4.4459,  4.4459,  4.4459],\n",
       "          [ 4.0902,  4.0902,  4.2680,  ...,  4.4459,  4.4459,  4.4459]],\n",
       "\n",
       "         [[-1.6005, -1.6005, -1.6005,  ..., -2.1340, -2.1340, -2.3118],\n",
       "          [-1.7783, -1.7783, -1.7783,  ..., -2.1340, -2.3118, -2.3118],\n",
       "          [-1.9562, -1.9562, -1.9562,  ..., -2.3118, -2.3118, -2.3118],\n",
       "          ...,\n",
       "          [-2.6675, -2.6675, -2.8454,  ..., -2.3118, -2.1340, -2.1340],\n",
       "          [-2.6675, -2.6675, -2.6675,  ..., -2.3118, -2.1340, -2.1340],\n",
       "          [-2.6675, -2.6675, -2.6675,  ..., -2.3118, -2.1340, -1.9562]],\n",
       "\n",
       "         [[ 0.1778,  0.1778,  0.1778,  ..., -0.3557, -0.3557, -0.5335],\n",
       "          [ 0.3557,  0.3557,  0.3557,  ..., -0.1778, -0.3557, -0.3557],\n",
       "          [ 0.3557,  0.3557,  0.3557,  ..., -0.1778, -0.1778, -0.1778],\n",
       "          ...,\n",
       "          [-0.1778, -0.3557, -0.3557,  ...,  0.0000,  0.0000,  0.1778],\n",
       "          [-0.1778, -0.3557, -0.3557,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.1778, -0.3557, -0.3557,  ..., -0.1778,  0.0000,  0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000,  0.0000, -0.1778,  ..., -0.1778,  0.0000,  0.1778],\n",
       "          [ 0.0000, -0.1778, -0.1778,  ..., -0.1778,  0.0000,  0.1778],\n",
       "          [-0.1778, -0.1778, -0.3557,  ..., -0.1778,  0.0000,  0.1778],\n",
       "          ...,\n",
       "          [ 0.1778,  0.1778,  0.1778,  ..., -0.1778, -0.1778, -0.1778],\n",
       "          [ 0.1778,  0.1778,  0.1778,  ..., -0.1778, -0.1778, -0.1778],\n",
       "          [ 0.1778,  0.1778,  0.1778,  ..., -0.1778, -0.1778, -0.1778]],\n",
       "\n",
       "         [[-1.4227, -1.4227, -1.4227,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-1.4227, -1.4227, -1.6005,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-1.4227, -1.6005, -1.6005,  ..., -1.9562, -1.7783, -1.7783],\n",
       "          ...,\n",
       "          [-1.0670, -1.0670, -1.0670,  ..., -1.0670, -1.0670, -1.0670],\n",
       "          [-1.0670, -1.0670, -1.0670,  ..., -1.0670, -1.0670, -0.8892],\n",
       "          [-1.0670, -1.0670, -1.0670,  ..., -1.0670, -0.8892, -0.8892]],\n",
       "\n",
       "         [[-1.9562, -2.1340, -2.1340,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-1.9562, -2.1340, -2.1340,  ..., -1.9562, -1.9562, -1.9562],\n",
       "          [-1.9562, -2.1340, -2.1340,  ..., -1.9562, -2.1340, -2.1340],\n",
       "          ...,\n",
       "          [-1.7783, -1.7783, -1.6005,  ..., -1.6005, -1.7783, -1.7783],\n",
       "          [-1.7783, -1.7783, -1.6005,  ..., -1.7783, -1.7783, -1.9562],\n",
       "          [-1.7783, -1.7783, -1.6005,  ..., -1.7783, -1.9562, -1.9562]]],\n",
       "\n",
       "\n",
       "        [[[ 4.2680,  4.6237,  4.8015,  ...,  4.8015,  4.6237,  4.4459],\n",
       "          [ 4.2680,  4.6237,  4.9794,  ...,  4.8015,  4.6237,  4.4459],\n",
       "          [ 4.4459,  4.6237,  4.9794,  ...,  4.8015,  4.6237,  4.4459],\n",
       "          ...,\n",
       "          [ 4.2680,  4.4459,  4.4459,  ...,  4.4459,  4.4459,  4.4459],\n",
       "          [ 4.2680,  4.4459,  4.4459,  ...,  4.4459,  4.4459,  4.2680],\n",
       "          [ 4.2680,  4.4459,  4.4459,  ...,  4.4459,  4.4459,  4.2680]],\n",
       "\n",
       "         [[-1.6005, -1.6005, -1.6005,  ..., -2.1340, -2.1340, -2.3118],\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -2.1340, -2.1340, -2.3118],\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -2.1340, -2.1340, -2.3118],\n",
       "          ...,\n",
       "          [-2.6675, -2.6675, -2.6675,  ..., -2.4897, -2.4897, -2.3118],\n",
       "          [-2.4897, -2.4897, -2.4897,  ..., -2.4897, -2.4897, -2.4897],\n",
       "          [-2.4897, -2.4897, -2.4897,  ..., -2.4897, -2.4897, -2.4897]],\n",
       "\n",
       "         [[ 0.7113,  0.7113,  0.7113,  ..., -0.3557, -0.3557, -0.5335],\n",
       "          [ 0.7113,  0.7113,  0.7113,  ..., -0.1778, -0.3557, -0.3557],\n",
       "          [ 0.7113,  0.7113,  0.7113,  ...,  0.0000, -0.1778, -0.1778],\n",
       "          ...,\n",
       "          [-0.5335, -0.5335, -0.5335,  ...,  0.1778,  0.0000,  0.0000],\n",
       "          [-0.5335, -0.7113, -0.7113,  ...,  0.0000,  0.0000, -0.1778],\n",
       "          [-0.7113, -0.7113, -0.7113,  ..., -0.1778, -0.1778, -0.1778]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1778,  0.1778,  0.0000,  ..., -0.1778,  0.0000,  0.0000],\n",
       "          [ 0.1778,  0.0000,  0.0000,  ..., -0.1778,  0.0000,  0.0000],\n",
       "          [ 0.1778,  0.0000, -0.1778,  ..., -0.1778,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.1778,  0.1778,  0.0000,  ...,  0.0000,  0.0000,  0.1778],\n",
       "          [ 0.1778,  0.1778,  0.0000,  ...,  0.0000,  0.0000,  0.1778],\n",
       "          [ 0.1778,  0.1778,  0.0000,  ...,  0.0000,  0.1778,  0.1778]],\n",
       "\n",
       "         [[-1.7783, -1.7783, -1.9562,  ..., -2.1340, -2.1340, -2.3118],\n",
       "          [-1.7783, -1.7783, -1.7783,  ..., -2.1340, -2.1340, -2.1340],\n",
       "          [-1.6005, -1.7783, -1.7783,  ..., -2.1340, -2.1340, -2.1340],\n",
       "          ...,\n",
       "          [-1.2448, -1.2448, -1.2448,  ..., -1.0670, -1.0670, -1.0670],\n",
       "          [-1.2448, -1.2448, -1.2448,  ..., -1.0670, -1.0670, -0.8892],\n",
       "          [-1.0670, -1.0670, -1.0670,  ..., -1.0670, -1.0670, -0.8892]],\n",
       "\n",
       "         [[-2.4897, -2.4897, -2.4897,  ..., -1.9562, -1.9562, -1.9562],\n",
       "          [-2.3118, -2.3118, -2.3118,  ..., -2.1340, -2.1340, -2.1340],\n",
       "          [-2.3118, -2.3118, -2.3118,  ..., -2.1340, -2.1340, -2.3118],\n",
       "          ...,\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -1.6005, -1.6005, -1.6005],\n",
       "          [-1.7783, -1.6005, -1.6005,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-1.7783, -1.7783, -1.6005,  ..., -1.7783, -1.7783, -1.7783]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save.encoder.conv1[0].weight\n",
    "model = CamVidModel(\"FPN\", \"resnext50_32x4d\", in_channels=3, out_classes=OUT_CLASSES)\n",
    "# model_loaded = torch.nn.Sequential(*(list(model_loaded.children())[:-1]))\n",
    "input, gt = next(iter(valid_loader))\n",
    "model.model = save\n",
    "\n",
    "model(input.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, gt = next(iter(valid_loader))\n",
    "image_prepared = (input - model.mean) / model.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 4.0902,  4.2680,  4.4459,  ...,  4.6237,  4.4459,  4.2680],\n",
       "          [ 4.0902,  4.2680,  4.6237,  ...,  4.8015,  4.6237,  4.4459],\n",
       "          [ 4.0902,  4.4459,  4.6237,  ...,  4.8015,  4.6237,  4.4459],\n",
       "          ...,\n",
       "          [ 3.9124,  4.0902,  4.2680,  ...,  4.2680,  4.2680,  4.0902],\n",
       "          [ 3.9124,  4.0902,  4.2680,  ...,  4.4459,  4.2680,  4.2680],\n",
       "          [ 4.0902,  4.0902,  4.2680,  ...,  4.4459,  4.4459,  4.2680]],\n",
       "\n",
       "         [[-1.6005, -1.6005, -1.6005,  ..., -1.9562, -2.1340, -2.3118],\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -2.1340, -2.1340, -2.3118],\n",
       "          [-1.6005, -1.7783, -1.7783,  ..., -2.1340, -2.3118, -2.3118],\n",
       "          ...,\n",
       "          [-2.8454, -2.8454, -2.8454,  ..., -2.4897, -2.4897, -2.3118],\n",
       "          [-2.6675, -2.6675, -2.6675,  ..., -2.4897, -2.3118, -2.3118],\n",
       "          [-2.6675, -2.6675, -2.6675,  ..., -2.4897, -2.3118, -2.3118]],\n",
       "\n",
       "         [[ 0.5335,  0.5335,  0.5335,  ..., -0.3557, -0.3557, -0.5335],\n",
       "          [ 0.5335,  0.5335,  0.5335,  ..., -0.1778, -0.3557, -0.3557],\n",
       "          [ 0.5335,  0.5335,  0.5335,  ..., -0.1778, -0.1778, -0.3557],\n",
       "          ...,\n",
       "          [-0.5335, -0.5335, -0.5335,  ..., -0.1778, -0.1778, -0.1778],\n",
       "          [-0.5335, -0.5335, -0.5335,  ..., -0.3557, -0.3557, -0.3557],\n",
       "          [-0.5335, -0.5335, -0.5335,  ..., -0.5335, -0.5335, -0.5335]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.1778, -0.3557, -0.3557,  ..., -0.1778,  0.0000,  0.1778],\n",
       "          [-0.3557, -0.3557, -0.3557,  ..., -0.1778,  0.0000,  0.1778],\n",
       "          [-0.3557, -0.3557, -0.5335,  ..., -0.1778, -0.1778,  0.0000],\n",
       "          ...,\n",
       "          [ 0.3557,  0.1778,  0.1778,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.3557,  0.3557,  0.1778,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.5335,  0.3557,  0.1778,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-1.6005, -1.6005, -1.4227,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-1.4227, -1.4227, -1.4227,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-1.4227, -1.2448, -1.2448,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          ...,\n",
       "          [-1.2448, -1.2448, -1.4227,  ..., -1.0670, -1.0670, -1.0670],\n",
       "          [-1.2448, -1.4227, -1.4227,  ..., -1.0670, -1.0670, -0.8892],\n",
       "          [-1.4227, -1.4227, -1.4227,  ..., -1.0670, -1.0670, -0.8892]],\n",
       "\n",
       "         [[-2.3118, -2.3118, -2.3118,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-2.3118, -2.3118, -2.3118,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-2.3118, -2.3118, -2.3118,  ..., -1.9562, -1.9562, -1.9562],\n",
       "          ...,\n",
       "          [-1.6005, -1.4227, -1.4227,  ..., -1.6005, -1.6005, -1.6005],\n",
       "          [-1.6005, -1.4227, -1.4227,  ..., -1.6005, -1.6005, -1.6005],\n",
       "          [-1.6005, -1.6005, -1.4227,  ..., -1.6005, -1.6005, -1.6005]]],\n",
       "\n",
       "\n",
       "        [[[ 4.4459,  4.6237,  4.8015,  ...,  4.6237,  4.4459,  4.2680],\n",
       "          [ 4.6237,  4.6237,  4.8015,  ...,  4.6237,  4.4459,  4.2680],\n",
       "          [ 4.6237,  4.8015,  4.9794,  ...,  4.6237,  4.4459,  4.2680],\n",
       "          ...,\n",
       "          [ 4.8015,  4.8015,  4.8015,  ...,  4.4459,  4.4459,  4.4459],\n",
       "          [ 4.8015,  4.8015,  4.8015,  ...,  4.4459,  4.4459,  4.4459],\n",
       "          [ 4.8015,  4.8015,  4.8015,  ...,  4.4459,  4.4459,  4.4459]],\n",
       "\n",
       "         [[-1.6005, -1.6005, -1.6005,  ..., -1.9562, -2.1340, -2.3118],\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -2.1340, -2.1340, -2.3118],\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -2.1340, -2.3118, -2.3118],\n",
       "          ...,\n",
       "          [-2.6675, -2.6675, -2.6675,  ..., -2.6675, -2.4897, -2.4897],\n",
       "          [-2.4897, -2.4897, -2.4897,  ..., -2.6675, -2.4897, -2.4897],\n",
       "          [-2.4897, -2.4897, -2.4897,  ..., -2.6675, -2.4897, -2.4897]],\n",
       "\n",
       "         [[ 0.7113,  0.5335,  0.5335,  ..., -0.1778, -0.1778, -0.1778],\n",
       "          [ 0.7113,  0.7113,  0.5335,  ...,  0.0000,  0.0000, -0.1778],\n",
       "          [ 0.7113,  0.7113,  0.7113,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [-0.3557, -0.3557, -0.3557,  ...,  0.0000,  0.0000,  0.1778],\n",
       "          [-0.3557, -0.3557, -0.3557,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.5335, -0.5335, -0.5335,  ..., -0.1778,  0.0000,  0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1778,  0.1778,  0.1778,  ..., -0.1778, -0.1778,  0.0000],\n",
       "          [ 0.1778,  0.0000,  0.0000,  ..., -0.3557, -0.1778, -0.1778],\n",
       "          [ 0.0000,  0.0000, -0.1778,  ..., -0.3557, -0.3557, -0.1778],\n",
       "          ...,\n",
       "          [ 0.1778,  0.1778,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.1778,  0.1778,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.1778,  0.1778,  0.1778,  ..., -0.1778,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-1.6005, -1.6005, -1.6005,  ..., -1.9562, -1.9562, -1.9562],\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -1.9562, -1.9562, -1.9562],\n",
       "          [-1.4227, -1.6005, -1.6005,  ..., -1.9562, -1.9562, -1.9562],\n",
       "          ...,\n",
       "          [-1.0670, -1.0670, -1.0670,  ..., -0.7113, -0.7113, -0.7113],\n",
       "          [-1.0670, -1.0670, -1.0670,  ..., -0.7113, -0.7113, -0.7113],\n",
       "          [-1.0670, -1.0670, -1.0670,  ..., -0.7113, -0.7113, -0.7113]],\n",
       "\n",
       "         [[-2.3118, -2.3118, -2.3118,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-2.3118, -2.3118, -2.1340,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-2.3118, -2.1340, -2.1340,  ..., -1.7783, -1.9562, -1.9562],\n",
       "          ...,\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -1.9562, -1.7783, -1.7783]]],\n",
       "\n",
       "\n",
       "        [[[ 4.2680,  4.4459,  4.4459,  ...,  4.8015,  4.6237,  4.4459],\n",
       "          [ 4.2680,  4.4459,  4.6237,  ...,  4.8015,  4.8015,  4.6237],\n",
       "          [ 4.0902,  4.4459,  4.6237,  ...,  4.9794,  4.8015,  4.6237],\n",
       "          ...,\n",
       "          [ 4.0902,  4.0902,  4.2680,  ...,  4.2680,  4.2680,  4.2680],\n",
       "          [ 4.0902,  4.0902,  4.2680,  ...,  4.4459,  4.4459,  4.4459],\n",
       "          [ 4.0902,  4.0902,  4.2680,  ...,  4.4459,  4.4459,  4.4459]],\n",
       "\n",
       "         [[-1.6005, -1.6005, -1.6005,  ..., -2.1340, -2.1340, -2.3118],\n",
       "          [-1.7783, -1.7783, -1.7783,  ..., -2.1340, -2.3118, -2.3118],\n",
       "          [-1.9562, -1.9562, -1.9562,  ..., -2.3118, -2.3118, -2.3118],\n",
       "          ...,\n",
       "          [-2.6675, -2.6675, -2.8454,  ..., -2.3118, -2.1340, -2.1340],\n",
       "          [-2.6675, -2.6675, -2.6675,  ..., -2.3118, -2.1340, -2.1340],\n",
       "          [-2.6675, -2.6675, -2.6675,  ..., -2.3118, -2.1340, -1.9562]],\n",
       "\n",
       "         [[ 0.1778,  0.1778,  0.1778,  ..., -0.3557, -0.3557, -0.5335],\n",
       "          [ 0.3557,  0.3557,  0.3557,  ..., -0.1778, -0.3557, -0.3557],\n",
       "          [ 0.3557,  0.3557,  0.3557,  ..., -0.1778, -0.1778, -0.1778],\n",
       "          ...,\n",
       "          [-0.1778, -0.3557, -0.3557,  ...,  0.0000,  0.0000,  0.1778],\n",
       "          [-0.1778, -0.3557, -0.3557,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.1778, -0.3557, -0.3557,  ..., -0.1778,  0.0000,  0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000,  0.0000, -0.1778,  ..., -0.1778,  0.0000,  0.1778],\n",
       "          [ 0.0000, -0.1778, -0.1778,  ..., -0.1778,  0.0000,  0.1778],\n",
       "          [-0.1778, -0.1778, -0.3557,  ..., -0.1778,  0.0000,  0.1778],\n",
       "          ...,\n",
       "          [ 0.1778,  0.1778,  0.1778,  ..., -0.1778, -0.1778, -0.1778],\n",
       "          [ 0.1778,  0.1778,  0.1778,  ..., -0.1778, -0.1778, -0.1778],\n",
       "          [ 0.1778,  0.1778,  0.1778,  ..., -0.1778, -0.1778, -0.1778]],\n",
       "\n",
       "         [[-1.4227, -1.4227, -1.4227,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-1.4227, -1.4227, -1.6005,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-1.4227, -1.6005, -1.6005,  ..., -1.9562, -1.7783, -1.7783],\n",
       "          ...,\n",
       "          [-1.0670, -1.0670, -1.0670,  ..., -1.0670, -1.0670, -1.0670],\n",
       "          [-1.0670, -1.0670, -1.0670,  ..., -1.0670, -1.0670, -0.8892],\n",
       "          [-1.0670, -1.0670, -1.0670,  ..., -1.0670, -0.8892, -0.8892]],\n",
       "\n",
       "         [[-1.9562, -2.1340, -2.1340,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-1.9562, -2.1340, -2.1340,  ..., -1.9562, -1.9562, -1.9562],\n",
       "          [-1.9562, -2.1340, -2.1340,  ..., -1.9562, -2.1340, -2.1340],\n",
       "          ...,\n",
       "          [-1.7783, -1.7783, -1.6005,  ..., -1.6005, -1.7783, -1.7783],\n",
       "          [-1.7783, -1.7783, -1.6005,  ..., -1.7783, -1.7783, -1.9562],\n",
       "          [-1.7783, -1.7783, -1.6005,  ..., -1.7783, -1.9562, -1.9562]]],\n",
       "\n",
       "\n",
       "        [[[ 4.2680,  4.6237,  4.8015,  ...,  4.8015,  4.6237,  4.4459],\n",
       "          [ 4.2680,  4.6237,  4.9794,  ...,  4.8015,  4.6237,  4.4459],\n",
       "          [ 4.4459,  4.6237,  4.9794,  ...,  4.8015,  4.6237,  4.4459],\n",
       "          ...,\n",
       "          [ 4.2680,  4.4459,  4.4459,  ...,  4.4459,  4.4459,  4.4459],\n",
       "          [ 4.2680,  4.4459,  4.4459,  ...,  4.4459,  4.4459,  4.2680],\n",
       "          [ 4.2680,  4.4459,  4.4459,  ...,  4.4459,  4.4459,  4.2680]],\n",
       "\n",
       "         [[-1.6005, -1.6005, -1.6005,  ..., -2.1340, -2.1340, -2.3118],\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -2.1340, -2.1340, -2.3118],\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -2.1340, -2.1340, -2.3118],\n",
       "          ...,\n",
       "          [-2.6675, -2.6675, -2.6675,  ..., -2.4897, -2.4897, -2.3118],\n",
       "          [-2.4897, -2.4897, -2.4897,  ..., -2.4897, -2.4897, -2.4897],\n",
       "          [-2.4897, -2.4897, -2.4897,  ..., -2.4897, -2.4897, -2.4897]],\n",
       "\n",
       "         [[ 0.7113,  0.7113,  0.7113,  ..., -0.3557, -0.3557, -0.5335],\n",
       "          [ 0.7113,  0.7113,  0.7113,  ..., -0.1778, -0.3557, -0.3557],\n",
       "          [ 0.7113,  0.7113,  0.7113,  ...,  0.0000, -0.1778, -0.1778],\n",
       "          ...,\n",
       "          [-0.5335, -0.5335, -0.5335,  ...,  0.1778,  0.0000,  0.0000],\n",
       "          [-0.5335, -0.7113, -0.7113,  ...,  0.0000,  0.0000, -0.1778],\n",
       "          [-0.7113, -0.7113, -0.7113,  ..., -0.1778, -0.1778, -0.1778]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1778,  0.1778,  0.0000,  ..., -0.1778,  0.0000,  0.0000],\n",
       "          [ 0.1778,  0.0000,  0.0000,  ..., -0.1778,  0.0000,  0.0000],\n",
       "          [ 0.1778,  0.0000, -0.1778,  ..., -0.1778,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.1778,  0.1778,  0.0000,  ...,  0.0000,  0.0000,  0.1778],\n",
       "          [ 0.1778,  0.1778,  0.0000,  ...,  0.0000,  0.0000,  0.1778],\n",
       "          [ 0.1778,  0.1778,  0.0000,  ...,  0.0000,  0.1778,  0.1778]],\n",
       "\n",
       "         [[-1.7783, -1.7783, -1.9562,  ..., -2.1340, -2.1340, -2.3118],\n",
       "          [-1.7783, -1.7783, -1.7783,  ..., -2.1340, -2.1340, -2.1340],\n",
       "          [-1.6005, -1.7783, -1.7783,  ..., -2.1340, -2.1340, -2.1340],\n",
       "          ...,\n",
       "          [-1.2448, -1.2448, -1.2448,  ..., -1.0670, -1.0670, -1.0670],\n",
       "          [-1.2448, -1.2448, -1.2448,  ..., -1.0670, -1.0670, -0.8892],\n",
       "          [-1.0670, -1.0670, -1.0670,  ..., -1.0670, -1.0670, -0.8892]],\n",
       "\n",
       "         [[-2.4897, -2.4897, -2.4897,  ..., -1.9562, -1.9562, -1.9562],\n",
       "          [-2.3118, -2.3118, -2.3118,  ..., -2.1340, -2.1340, -2.1340],\n",
       "          [-2.3118, -2.3118, -2.3118,  ..., -2.1340, -2.1340, -2.3118],\n",
       "          ...,\n",
       "          [-1.6005, -1.6005, -1.6005,  ..., -1.6005, -1.6005, -1.6005],\n",
       "          [-1.7783, -1.6005, -1.6005,  ..., -1.7783, -1.7783, -1.7783],\n",
       "          [-1.7783, -1.7783, -1.6005,  ..., -1.7783, -1.7783, -1.7783]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save(image_prepared.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "input model must be a GraphModule, Got type:<class 'src.models.camvid_segmentation_multiclass.CamVidModel'> Please make sure to follow the tutorials.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m prepared_model\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m      3\u001b[0m prepared_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 4\u001b[0m save \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/quantize_fx.py:596\u001b[0m, in \u001b[0;36mconvert_fx\u001b[0;34m(graph_module, convert_custom_config, _remove_qconfig, qconfig_mapping, backend_config)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\" Convert a calibrated or trained model to a quantized model\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    593\u001b[0m \n\u001b[1;32m    594\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    595\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_api.quantize_fx.convert_fx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_custom_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_remove_qconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_remove_qconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/quantize_fx.py:520\u001b[0m, in \u001b[0;36m_convert_fx\u001b[0;34m(graph_module, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig, qconfig_mapping, backend_config, is_decomposed)\u001b[0m\n\u001b[1;32m    515\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    516\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a convert_custom_config_dict to convert is deprecated and will not be supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a future version. Please pass in a ConvertCustomConfig instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    518\u001b[0m     convert_custom_config \u001b[38;5;241m=\u001b[39m ConvertCustomConfig\u001b[38;5;241m.\u001b[39mfrom_dict(convert_custom_config)\n\u001b[0;32m--> 520\u001b[0m \u001b[43m_check_is_graph_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m preserved_attr_names \u001b[38;5;241m=\u001b[39m convert_custom_config\u001b[38;5;241m.\u001b[39mpreserved_attributes\n\u001b[1;32m    522\u001b[0m preserved_attrs \u001b[38;5;241m=\u001b[39m {attr: \u001b[38;5;28mgetattr\u001b[39m(graph_module, attr) \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m preserved_attr_names \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(graph_module, attr)}\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/quantize_fx.py:44\u001b[0m, in \u001b[0;36m_check_is_graph_module\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_is_graph_module\u001b[39m(model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, GraphModule):\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput model must be a GraphModule, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot type:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(model))\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please make \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msure to follow the tutorials.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: input model must be a GraphModule, Got type:<class 'src.models.camvid_segmentation_multiclass.CamVidModel'> Please make sure to follow the tutorials."
     ]
    }
   ],
   "source": [
    "# when dont quantize some layers\n",
    "prepared_model.cpu()\n",
    "prepared_model.eval()\n",
    "save = convert_fx(prepared_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 1.,  1.,  1.,  ...,  0.,  0.,  0.],\n",
       "          [ 1.,  1.,  1.,  ...,  0.,  0.,  0.],\n",
       "          [ 1.,  1.,  2.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  7., 10., 13.],\n",
       "          [ 0.,  0.,  0.,  ...,  6.,  9., 12.],\n",
       "          [ 0.,  0.,  0.,  ...,  5.,  8., 10.],\n",
       "          ...,\n",
       "          [ 2.,  3.,  3.,  ...,  0.,  0.,  0.],\n",
       "          [ 2.,  3.,  3.,  ...,  0.,  0.,  0.],\n",
       "          [ 2.,  2.,  3.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  1.,  2.,  2.],\n",
       "          [ 0.,  0.,  0.,  ...,  1.,  1.,  2.],\n",
       "          [ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  4.,  6.,  8.],\n",
       "          [ 0.,  0.,  0.,  ...,  6.,  9., 12.],\n",
       "          [ 0.,  0.,  0.,  ...,  8., 12., 16.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  ...,  0.,  0.,  0.],\n",
       "          [ 1.,  1.,  1.,  ...,  0.,  0.,  0.],\n",
       "          [ 2.,  1.,  1.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  ...,  1.,  1.,  0.],\n",
       "          [ 1.,  1.,  0.,  ...,  1.,  1.,  0.],\n",
       "          [ 1.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 3.,  3.,  2.,  ...,  0.,  0.,  0.],\n",
       "          [ 4.,  3.,  3.,  ...,  0.,  0.,  0.],\n",
       "          [ 5.,  4.,  4.,  ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "        [[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  1.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  1.,  1.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  1.,  1.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  8., 11., 15.],\n",
       "          [ 0.,  0.,  0.,  ...,  7., 10., 13.],\n",
       "          [ 0.,  0.,  0.,  ...,  6.,  9., 12.],\n",
       "          ...,\n",
       "          [ 3.,  3.,  3.,  ...,  0.,  0.,  0.],\n",
       "          [ 2.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
       "          [ 2.,  2.,  2.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  ...,  0.,  0.,  0.],\n",
       "          [ 1.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 1.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  1.,  2.,  2.],\n",
       "          [ 0.,  0.,  0.,  ...,  1.,  1.,  2.],\n",
       "          [ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  4.,  6.,  8.],\n",
       "          [ 0.,  0.,  0.,  ...,  6.,  8., 11.],\n",
       "          [ 0.,  0.,  0.,  ...,  8., 11., 15.]],\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  1.,  1.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  1.,  1.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 4.,  3.,  2.,  ...,  0.,  0.,  0.],\n",
       "          [ 4.,  3.,  2.,  ...,  0.,  0.,  0.],\n",
       "          [ 5.,  4.,  3.,  ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "        [[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 2.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
       "          [ 2.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
       "          [ 2.,  2.,  2.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  7., 11., 14.],\n",
       "          [ 0.,  0.,  0.,  ...,  6.,  9., 12.],\n",
       "          [ 0.,  0.,  0.,  ...,  5.,  8., 11.],\n",
       "          ...,\n",
       "          [ 3.,  3.,  3.,  ...,  0.,  0.,  0.],\n",
       "          [ 3.,  3.,  2.,  ...,  0.,  0.,  0.],\n",
       "          [ 3.,  3.,  2.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  1.,  1.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  1.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  1.,  ...,  4.,  5.,  7.],\n",
       "          [ 0.,  0.,  1.,  ...,  5.,  8., 11.],\n",
       "          [ 0.,  0.,  1.,  ...,  7., 11., 14.]],\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  1.,  1.,  0.],\n",
       "          [ 1.,  1.,  0.,  ...,  1.,  1.,  0.],\n",
       "          ...,\n",
       "          [ 1.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         [[ 2.,  2.,  1.,  ...,  1.,  1.,  0.],\n",
       "          [ 2.,  1.,  1.,  ...,  1.,  1.,  0.],\n",
       "          [ 2.,  1.,  1.,  ...,  1.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 3.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
       "          [ 3.,  3.,  2.,  ...,  0.,  0.,  0.],\n",
       "          [ 4.,  3.,  2.,  ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "        [[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  1.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  1.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  1.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  7., 11., 14.],\n",
       "          [ 0.,  0.,  0.,  ...,  6.,  9., 12.],\n",
       "          [ 0.,  0.,  0.,  ...,  5.,  8., 11.],\n",
       "          ...,\n",
       "          [ 2.,  2.,  2.,  ...,  1.,  1.,  0.],\n",
       "          [ 2.,  2.,  2.,  ...,  1.,  0.,  0.],\n",
       "          [ 2.,  2.,  1.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  ...,  0.,  0.,  0.],\n",
       "          [ 1.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 1.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  1.,  1.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  1.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  1.,  ...,  4.,  5.,  7.],\n",
       "          [ 0.,  1.,  1.,  ...,  5.,  8., 11.],\n",
       "          [ 0.,  1.,  1.,  ...,  7., 11., 14.]],\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         [[ 1.,  1.,  1.,  ...,  1.,  1.,  0.],\n",
       "          [ 1.,  1.,  0.,  ...,  1.,  1.,  0.],\n",
       "          [ 1.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 2.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
       "          [ 3.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
       "          [ 4.,  3.,  2.,  ...,  0.,  0.,  0.]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcpu = latency_cpu(save, input, warmup_n=10, benchmark_n=30)\n",
    "lcpu_p2 = latency_cpu_profiler(save, input, warmup_n=10, benchmark_n=30)\n",
    "\n",
    "print(\n",
    "    lcpu_p1.key_averages(group_by_input_shape=True).table(\n",
    "        sort_by=\"cpu_time_total\", row_limit=5\n",
    "    )\n",
    ")\n",
    "print(\"\\n--- after  convert_fx ---\")\n",
    "print(\n",
    "    lcpu_p2.key_averages(group_by_input_shape=True).table(\n",
    "        sort_by=\"cpu_time_total\", row_limit=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `GraphModule.__new__.<locals>.GraphModuleImpl`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprepared_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py:553\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    509\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     ckpt_path: Optional[_PATH] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Runs the full optimization routine.\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \n\u001b[1;32m    517\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m \n\u001b[1;32m    552\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_unwrap_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    555\u001b[0m     _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/compile.py:111\u001b[0m, in \u001b[0;36m_maybe_unwrap_optimized\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m    110\u001b[0m _check_mixed_imports(model)\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `GraphModule.__new__.<locals>.GraphModuleImpl`"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer.model.eval()\n",
    "with torch.no_grad():\n",
    "    save_load_torch_model(trainer.model, path=\"camvid_model_int8.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:1207: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- after  convert_fx ---\n",
      "Start CPU benchmark with input shape: torch.Size([4, 3, 384, 480]) cpu\n",
      "939.557ms +- 44.068ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-10-31 13:30:57 649:649 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                                                                      Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                  model_inference         2.53%      86.740ms       100.00%        3.435s        3.435s           0 b      -2.11 Gb             1                                                                                []  \n",
      "                     aten::conv2d         0.00%       8.000us         5.86%     201.284ms     201.284ms      22.50 Mb           0 b             1                         [[4, 256, 96, 120], [128, 256, 3, 3], [], [], [], [], []]  \n",
      "                aten::convolution         0.00%      38.000us         5.86%     201.276ms     201.276ms      22.50 Mb           0 b             1                 [[4, 256, 96, 120], [128, 256, 3, 3], [], [], [], [], [], [], []]  \n",
      "               aten::_convolution         0.00%      17.000us         5.86%     201.238ms     201.238ms      22.50 Mb           0 b             1  [[4, 256, 96, 120], [128, 256, 3, 3], [], [], [], [], [], [], [], [], [], [], []  \n",
      "         aten::mkldnn_convolution         5.86%     201.160ms         5.86%     201.221ms     201.221ms      22.50 Mb           0 b             1                         [[4, 256, 96, 120], [128, 256, 3, 3], [], [], [], [], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "Self CPU time total: 3.435s\n",
      "\n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                                               Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------  \n",
      "                  model_inference         3.34%      32.712ms       100.00%     978.130ms     978.130ms           0 b    -691.00 Mb             1                                                         []  \n",
      "                        aten::cat         5.07%      49.570ms        13.34%     130.516ms      65.258ms     112.50 Mb     -90.00 Mb             2                                                   [[], []]  \n",
      "                quantized::conv2d        11.98%     117.193ms        12.00%     117.342ms      39.114ms      22.50 Mb     -90.00 Mb             3                            [[4, 256, 96, 120], [], [], []]  \n",
      "                      aten::stack         0.01%      59.000us         8.80%      86.033ms      86.033ms      22.50 Mb           0 b             1                                                   [[], []]  \n",
      "           quantized::conv2d_relu         7.64%      74.747ms         7.67%      75.034ms      10.719ms      10.55 Mb     -42.19 Mb             7                            [[4, 1024, 24, 30], [], [], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------  \n",
      "Self CPU time total: 978.130ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-10-31 13:30:58 649:649 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-10-31 13:30:58 649:649 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "# when dont quantize some layers\n",
    "prepared_model.cpu()\n",
    "save = convert_fx(prepared_model)\n",
    "\n",
    "lcpu = latency_cpu(save, input, warmup_n=10, benchmark_n=30)\n",
    "lcpu_p2 = latency_cpu_profiler(save, input, warmup_n=10, benchmark_n=30)\n",
    "\n",
    "print(\n",
    "    lcpu_p1.key_averages(group_by_input_shape=True).table(\n",
    "        sort_by=\"cpu_time_total\", row_limit=5\n",
    "    )\n",
    ")\n",
    "print(\"\\n--- after  convert_fx ---\")\n",
    "print(\n",
    "    lcpu_p2.key_averages(group_by_input_shape=True).table(\n",
    "        sort_by=\"cpu_time_total\", row_limit=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "942.21"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms = lcpu_p2.key_averages(group_by_input_shape=True).self_cpu_time_total / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- after  convert_fx ---\n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls                                               Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------  \n",
      "                  model_inference         3.62%      34.127ms       100.00%     942.210ms     942.210ms           0 b    -691.00 Mb             1                                                         []  \n",
      "                quantized::conv2d        12.40%     116.840ms        12.41%     116.928ms      38.976ms      22.50 Mb     -90.00 Mb             3                            [[4, 256, 96, 120], [], [], []]  \n",
      "                quantized::conv2d         9.51%      89.623ms         9.53%      89.787ms      17.957ms      23.91 Mb     -95.62 Mb             5                             [[4, 256, 48, 60], [], [], []]  \n",
      "           quantized::conv2d_relu         8.05%      75.829ms         8.07%      76.065ms      10.866ms      10.55 Mb     -42.19 Mb             7                            [[4, 1024, 24, 30], [], [], []]  \n",
      "                 aten::dequantize         2.59%      24.371ms         7.68%      72.350ms      72.350ms      90.00 Mb     -22.50 Mb             1                                     [[4, 4, 128, 96, 120]]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------  \n",
      "Self CPU time total: 942.210ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with basic config\n",
    "prepared_model.cpu()\n",
    "save = convert_fx(prepared_model)\n",
    "\n",
    "lcpu = latency_cpu(save, input, warmup_n=10, benchmark_n=30)\n",
    "lcpu_p2 = latency_cpu_profiler(save, input, warmup_n=10, benchmark_n=30)\n",
    "\n",
    "\n",
    "print(\"\\n--- after  convert_fx ---\")\n",
    "print(\n",
    "    lcpu_p2.key_averages(group_by_input_shape=True).table(\n",
    "        sort_by=\"cpu_time_total\", row_limit=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Replaced Conv Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.common.replace_conv_resnet import replace_conv2d_with_custom\n",
    "from copy import deepcopy\n",
    "\n",
    "model = CamVidModel(\"FPN\", \"resnext50_32x4d\", in_channels=3, out_classes=OUT_CLASSES)\n",
    "# model_loaded = torch.nn.Sequential(*(list(model_loaded.children())[:-1]))\n",
    "input, gt = next(iter(valid_loader))\n",
    "input = input[0][None, ...]\n",
    "\n",
    "input = input.cuda()\n",
    "model.cuda()\n",
    "forwarded = model(input)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "Conv2dImg2Col(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"
     ]
    }
   ],
   "source": [
    "model.cpu()\n",
    "print(model.model.encoder.conv1)\n",
    "replace_conv2d_with_custom(model)\n",
    "print(model.model.encoder.conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (11520x1152 and 36x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/conv2d_reimagined/src/models/camvid_segmentation_multiclass.py:62\u001b[0m, in \u001b[0;36mCamVidModel.forward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Normalize image\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     image \u001b[38;5;241m=\u001b[39m (image \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd\n\u001b[0;32m---> 62\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "File \u001b[0;32m/workspaces/conv2d_reimagined/src/models/camvid_segmentation_multiclass.py:48\u001b[0m, in \u001b[0;36mCamVidModel.forward_model\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder(features)\n\u001b[1;32m     51\u001b[0m     masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msegmentation_head(decoder_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/segmentation_models_pytorch/encoders/resnet.py:72\u001b[0m, in \u001b[0;36mResNetEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_depth \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     71\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m---> 72\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_depth \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py:150\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m    148\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m--> 150\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/conv2d_reimagined/experiments/common/conv2d_img2col.py:184\u001b[0m, in \u001b[0;36mConv2dImg2Col.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImg2ColConvFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m/workspaces/conv2d_reimagined/experiments/common/conv2d_img2col.py:50\u001b[0m, in \u001b[0;36mImg2ColConvFunction.forward\u001b[0;34m(ctx, X, weight, bias, stride, padding, padding_mode)\u001b[0m\n\u001b[1;32m     47\u001b[0m patches_reshaped \u001b[38;5;241m=\u001b[39m patches_reshaped\u001b[38;5;241m.\u001b[39mview(N \u001b[38;5;241m*\u001b[39m H_out \u001b[38;5;241m*\u001b[39m W_out, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m weight_reshaped \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mview(C_out, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_reshaped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bias\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (11520x1152 and 36x128)"
     ]
    }
   ],
   "source": [
    "model(input.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/conv2d_reimagined/src/models/camvid_segmentation_multiclass.py:61\u001b[0m, in \u001b[0;36mCamVidModel.forward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Normalize image\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     image \u001b[38;5;241m=\u001b[39m (\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd\n\u001b[1;32m     62\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_model(image)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (11520x1152 and 36x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 2\u001b[0m forwarded_custom \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(forwarded, forwarded_custom, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/conv2d_reimagined/src/models/camvid_segmentation_multiclass.py:62\u001b[0m, in \u001b[0;36mCamVidModel.forward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Normalize image\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     image \u001b[38;5;241m=\u001b[39m (image \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd\n\u001b[0;32m---> 62\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "File \u001b[0;32m/workspaces/conv2d_reimagined/src/models/camvid_segmentation_multiclass.py:48\u001b[0m, in \u001b[0;36mCamVidModel.forward_model\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder(features)\n\u001b[1;32m     51\u001b[0m     masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msegmentation_head(decoder_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/segmentation_models_pytorch/encoders/resnet.py:72\u001b[0m, in \u001b[0;36mResNetEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_depth \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     71\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m---> 72\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_depth \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py:150\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m    148\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m--> 150\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/conv2d_reimagined/experiments/common/conv2d_img2col.py:184\u001b[0m, in \u001b[0;36mConv2dImg2Col.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImg2ColConvFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m/workspaces/conv2d_reimagined/experiments/common/conv2d_img2col.py:50\u001b[0m, in \u001b[0;36mImg2ColConvFunction.forward\u001b[0;34m(ctx, X, weight, bias, stride, padding, padding_mode)\u001b[0m\n\u001b[1;32m     47\u001b[0m patches_reshaped \u001b[38;5;241m=\u001b[39m patches_reshaped\u001b[38;5;241m.\u001b[39mview(N \u001b[38;5;241m*\u001b[39m H_out \u001b[38;5;241m*\u001b[39m W_out, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m weight_reshaped \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mview(C_out, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_reshaped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bias\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (11520x1152 and 36x128)"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "forwarded_custom = model(input)\n",
    "\n",
    "assert torch.allclose(forwarded, forwarded_custom, atol=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2955, device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " tensor(1.5659, device='cuda:0', grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forwarded[0, 0, 0, 0], forwarded_custom[0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "Conv2dImg2Col(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (11520x1152 and 36x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_custom_conv\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_custom_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/conv2d_reimagined/src/models/camvid_segmentation_multiclass.py:62\u001b[0m, in \u001b[0;36mCamVidModel.forward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Normalize image\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     image \u001b[38;5;241m=\u001b[39m (image \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd\n\u001b[0;32m---> 62\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "File \u001b[0;32m/workspaces/conv2d_reimagined/src/models/camvid_segmentation_multiclass.py:48\u001b[0m, in \u001b[0;36mCamVidModel.forward_model\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder(features)\n\u001b[1;32m     51\u001b[0m     masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msegmentation_head(decoder_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/segmentation_models_pytorch/encoders/resnet.py:72\u001b[0m, in \u001b[0;36mResNetEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_depth \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     71\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m---> 72\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_depth \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py:150\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m    148\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m--> 150\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/conv2d_reimagined/experiments/common/conv2d_img2col.py:184\u001b[0m, in \u001b[0;36mConv2dImg2Col.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImg2ColConvFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m/workspaces/conv2d_reimagined/experiments/common/conv2d_img2col.py:50\u001b[0m, in \u001b[0;36mImg2ColConvFunction.forward\u001b[0;34m(ctx, X, weight, bias, stride, padding, padding_mode)\u001b[0m\n\u001b[1;32m     47\u001b[0m patches_reshaped \u001b[38;5;241m=\u001b[39m patches_reshaped\u001b[38;5;241m.\u001b[39mview(N \u001b[38;5;241m*\u001b[39m H_out \u001b[38;5;241m*\u001b[39m W_out, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m weight_reshaped \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mview(C_out, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_reshaped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bias\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (11520x1152 and 36x128)"
     ]
    }
   ],
   "source": [
    "model_custom_conv.cuda()\n",
    "model_custom_conv(input.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple eval latency with replaced conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (11520x1152 and 36x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lcpu \u001b[38;5;241m=\u001b[39m \u001b[43mlatency_cpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_custom_conv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbenchmark_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m lgpu \u001b[38;5;241m=\u001b[39m latency_gpu(model_custom_conv, \u001b[38;5;28minput\u001b[39m, warmup_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, benchmark_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      3\u001b[0m lcpu_p1 \u001b[38;5;241m=\u001b[39m latency_cpu_profiler(model_custom_conv, \u001b[38;5;28minput\u001b[39m, warmup_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, benchmark_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/conv2d_reimagined/src/core/latency.py:17\u001b[0m, in \u001b[0;36mlatency_cpu\u001b[0;34m(model, test_input, warmup_n, benchmark_n)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(warmup_n):\n\u001b[0;32m---> 17\u001b[0m             \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/conv2d_reimagined/src/models/camvid_segmentation_multiclass.py:62\u001b[0m, in \u001b[0;36mCamVidModel.forward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Normalize image\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     image \u001b[38;5;241m=\u001b[39m (image \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd\n\u001b[0;32m---> 62\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "File \u001b[0;32m/workspaces/conv2d_reimagined/src/models/camvid_segmentation_multiclass.py:48\u001b[0m, in \u001b[0;36mCamVidModel.forward_model\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder(features)\n\u001b[1;32m     51\u001b[0m     masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msegmentation_head(decoder_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/segmentation_models_pytorch/encoders/resnet.py:72\u001b[0m, in \u001b[0;36mResNetEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_depth \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     71\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m---> 72\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_depth \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py:150\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m    148\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m--> 150\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/conv2d_reimagined/experiments/common/conv2d_img2col.py:184\u001b[0m, in \u001b[0;36mConv2dImg2Col.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImg2ColConvFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m/workspaces/conv2d_reimagined/experiments/common/conv2d_img2col.py:50\u001b[0m, in \u001b[0;36mImg2ColConvFunction.forward\u001b[0;34m(ctx, X, weight, bias, stride, padding, padding_mode)\u001b[0m\n\u001b[1;32m     47\u001b[0m patches_reshaped \u001b[38;5;241m=\u001b[39m patches_reshaped\u001b[38;5;241m.\u001b[39mview(N \u001b[38;5;241m*\u001b[39m H_out \u001b[38;5;241m*\u001b[39m W_out, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m weight_reshaped \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mview(C_out, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_reshaped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bias\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (11520x1152 and 36x128)"
     ]
    }
   ],
   "source": [
    "lcpu = latency_cpu(model_custom_conv, input, warmup_n=2, benchmark_n=5)\n",
    "lgpu = latency_gpu(model_custom_conv, input, warmup_n=2, benchmark_n=5)\n",
    "lcpu_p1 = latency_cpu_profiler(model_custom_conv, input, warmup_n=2, benchmark_n=5)\n",
    "print(lcpu_p1.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=5))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
